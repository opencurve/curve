# Curve 测试环境配置信息

Curve 当前测试的性能，如果没有特别说明，都是在基于以下配置的机器上测试的。该配置仅仅表示curve当前性能数据的测试环境，由于部分硬件版本比较旧，该环境的硬件不作为推荐硬件型号，但是对硬件的配置部分可做参考。

## 集群topo

### curvebs 块存储测试集群规模

mds和chunkserver服务混合部署。

mds : 3台机器

chunkserver : 9台机器 * 20块盘/每台机器

## 机器选型

| 计算/存储节点机型 |
| --- |
| Dell Inc. PowerEdge R730xd |
| Inspur NF5280M4 |

## 所有机器公共配置

| 模块 | 版本 | 配置 |
| --- | --- | --- |
| 系统盘 |$ sudo /usr/sbin/megacli  -LDPDInfo -aAll <br>Number of Virtual Disks: 1<br>Virtual Drive: 0 (Target Id: 0)<br>Name :<br>RAID Level : Primary-1, Secondary-0, RAID Level Qualifier-0<br>Size : 1.090 TB<br>Sector Size : 512<br>Is VD emulated : No<br>Mirror Data : 1.090 TB<br>State : Optimal<br>Strip Size : 256 KB<br>Number Of Drives : 2<br>Span Depth : 1<br>Default Cache Policy: WriteBack, ReadAhead, Direct, Write Cache OK if<br>Bad BBU<br>Current Cache Policy: WriteBack, ReadAhead, Direct, Write Cache OK if<br>Bad BBU<br>Default Access Policy: Read/Write<br>Current Access Policy: Read/Write<br>Disk Cache Policy : Disabled<br>Encryption Type : None<br>Bad Blocks Exist: No<br>PI type: No PI |RAID0 |

## mds 节点

| 模块 | 版本 | 配置 |
| --- | --- | --- |
| 机型 | Dell PowerEdge R730xd |
| os | debain9 |
| 内核 | 4.9.65|
| cpu||设置performance，开启cpu性能最大化|
| 网卡| Bonding Mode: IEEE 802.3ad Dynamic link aggregation<br>Transmit Hash Policy: layer2+3 (2)<br>MII Status: up<br>MII Polling Interval (ms): 100<br>Up Delay (ms): 200<br>Down Delay (ms): 200 |万M网卡双网口组bond|
|系统盘||1.2T*2 SAS （RAID1模式）<br>初始化之后进行拷盘测试，提前发现并剔除坏盘、慢盘等异常|
|数据盘||1.8T*2 SSD (不需要RAID)<br>初始化之后进行拷盘测试，提前发现并剔除坏盘、慢盘等异常|


## chunkserver 节点

|模块|版本|配置|
|---|---|---|
|机型|inspur NF5280M4| |
|os|debian9| |
|内核|4.9.65| |
|cpu| |设置performance，开启cpu性能最大化|
|megaraid_sas| 06.811.02.00-rc4| |
|数据盘raid卡配置|1.RAID FW 版本：24.21.0-0061 <br>2.RAID卡驱动版本：06.811.02.00-rc4|1. 配置为JBOD模式<br> 2. RAID 卡关闭 consistent check<br>3.RAID卡缓存策略：WriteThrough|
|数据盘磁盘|数据盘类型：1.8TB * 20 Intel S4500<br>| 1.数据盘关闭磁盘缓存<br>2.磁盘调度策略，SSD盘noop，HDD盘deadline<br>// 初始化之后进行拷盘测试，提前发现并剔除坏盘、慢盘等异常 <br>3、 sudo tune2fs -m 0 /dev/sda8<br>释放文件系统预留的5%空间 <br>4、如果至少两个raid卡，每个raid卡上数据盘尽量均衡 |
| 网卡|bond配置（<br>Bonding Mode: IEEE 802.3ad Dynamic link aggregation<br>Transmit Hash Policy: layer2+3 (2)<br>MII Status: up<br>MII Polling Interval (ms): 100<br>Up Delay (ms): 200<br>Down Delay (ms): 200 ）|万M网卡双网口组bond|
系统盘|1.2T*2 SAS | RAID1模式|
|fd数量||1.系统允许的最大fd数量至少为10000000<br> 2.单个进程允许的最大fd数量不低于250000|
|mmap地址数量||每一个bthread都需要使用mmap分配stack，mmap数量操作系统是有限制的，建议修改/proc/sys/vm/max_map_count 至少为5642720。|

## 一些相关的命令

### raid卡查询命令

1. RAID FW 版本 查询命令：`sudo /usr/sbin/megacli -AdpAllInfo -aALL  | grep "FW Package Build"`
2. RAID 卡关闭 consistent check 查询命令：`sudo /usr/sbin/megacli  -AdpCcSched -info -a0`
3. RAID卡缓存策略查询命令：`sudo megacli -LDGetProp -Cache -LALL -a0`
4. RAID卡驱动版本查询命令：`sudo modinfo megaraid_sas`

### 磁盘相关命令

1. 关闭磁盘缓存命令：`sudo /sbin/hdparm -W 0 /dev/sdh`
2. 磁盘缓存是否关闭，查询命令：`sudo  /sbin/hdparm -W /dev/sdh`
3. 磁盘调度策略，查询命令：`cat /sys/block/sdxxx/queue/schedule`
4. 释放文件系统预留空空间命令：`sudo tune2fs -m 0 /dev/sda8`

### 修改单个进程的允许的最大fs数量

1. 查询系统允许的最大fd数量：`cat /proc/sys/fs/file-max`
2. 查询单个进程允许的最大fd数量：`ulimit -n`
3. 修改单个进程允许的最大fd数量：

```
修改/etc/sysctl.conf配置，然后执行sysctl -p 生效。
echo "fs.nr_open = 2500000" >> /etc/sysctl.conf
echo "fs.file-max = 40000000" >> /etc/sysctl.conf
```

