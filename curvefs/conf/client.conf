##### mdsOpt
# RPC total retry time with MDS
mdsOpt.mdsMaxRetryMS=16000
# The maximum timeout of RPC communicating with MDS.
# The timeout of exponential backoff cannot exceed this value
mdsOpt.rpcRetryOpt.maxRPCTimeoutMS=2000
# RPC timeout for once communication with MDS
mdsOpt.rpcRetryOpt.rpcTimeoutMs=500
# RPC with mds needs to sleep for a period of time before each retry
mdsOpt.rpcRetryOpt.rpcRetryIntervalUS=50000
# Switch if the number of consecutive retries on the current MDS exceeds the limit.
# The number of failures includes the number of timeout retries
mdsOpt.rpcRetryOpt.maxFailedTimesBeforeChangeAddr=2
# The normal retry times for trigger wait strategy
mdsOpt.rpcRetryOpt.normalRetryTimesBeforeTriggerWait=3
# Sleep interval for wait
mdsOpt.rpcRetryOpt.waitSleepMs=1000
mdsOpt.rpcRetryOpt.addrs=127.0.0.1:6700,127.0.0.1:6701,127.0.0.1:6702  # __ANSIBLE_TEMPLATE__ {{ groups.mds | join_peer(hostvars, "mds_listen_port") }} __ANSIBLE_TEMPLATE__

#### metaCacheOpt
# Gets the number of retries for the leader
metaCacheOpt.metacacheGetLeaderRetry=3
# Need to sleep for a period of time before each get leader retry
metaCacheOpt.metacacheRPCRetryIntervalUS=100000
# RPC timeout of get leader
metaCacheOpt.metacacheGetLeaderRPCTimeOutMS=1000

#### excutorOpt
# excutorOpt rpc with metaserver
# rpc retry times with metaserver
excutorOpt.maxRetry=1000000
# Retry sleep time between failed RPCs
excutorOpt.retryIntervalUS=500
# RPC timeout for communicating with metaserver
excutorOpt.rpcTimeoutMS=1000
# The maximum timeout RPC time of the retry request.
# The timeout time will follow the exponential backoff policy.
# Because the timeout occurs when the network is congested, the RPC timeout needs to be increased
excutorOpt.maxRPCTimeoutMS=8000
# Maximum sleep time between retry requests.
# when the network is congested or the metaserver is overloaded,
# it is necessary to increase the sleep time
excutorOpt.maxRetrySleepIntervalUS=8000000
excutorOpt.minRetryTimesForceTimeoutBackoff=5
excutorOpt.maxRetryTimesBeforeConsiderSuspend=20
excutorOpt.batchLimit=100

#### spaceserver
spaceserver.spaceaddr=127.0.0.1:19999  # __ANSIBLE_TEMPLATE__ {{ groups.space | join_peer(hostvars, "space_listen_port") }} __ANSIBLE_TEMPLATE__
spaceserver.rpcTimeoutMs=1000

#### bdev
bdev.confpath=/etc/curve/client.conf

#### extentManager
extentManager.preAllocSize=65536

#### brpc
# close socket after defer.close.second
rpc.defer.close.second=1
# rpc health check interval in second, 0 or negative value means disable health check
rpc.healthCheckIntervalSec=0

#### fuseClient
# TODO(xuchaojie): add unit
fuseClient.attrTimeOut=1.0
fuseClient.entryTimeOut=1.0
fuseClient.listDentryLimit=65536
fuseClient.flushPeriodSec=5
fuseClient.maxNameLength=255
fuseClient.iCacheLruSize=65536
fuseClient.dCacheLruSize=65536
fuseClient.enableICacheMetrics=true
fuseClient.enableDCacheMetrics=true
fuseClient.cto=true
# FuseOpFlush retry intervel, default is 2s
fuseClient.flushRetryIntervalMs=2000

#### volume
volume.bigFileSize=1048576
volume.volBlockSize=4096
volume.fsBlockSize=4096

#### s3
# the max size that fuse send
s3.fuseMaxSize=131072
s3.pagesize=65536
# prefetch blocks that disk cache use
s3.prefetchBlocks=1
# prefetch threads
s3.prefetchExecQueueNum=1
# start sleep when mem cache use ratio is greater than nearfullRatio,
# sleep time increase follow with mem cache use raito, baseSleepUs is baseline.
s3.nearfullRatio=70
s3.baseSleepUs=500

# TODO(huyao): use more meaningfull name
# background thread schedule time
s3.intervalSec=3
# data cache flush wait time
s3.flushIntervalSec=5
s3.writeCacheMaxByte=838860800
s3.readCacheMaxByte=209715200
# http = 0, https = 1
s3.http_scheme=0
s3.verify_SSL=False
s3.max_connections=32
s3.connect_timeout=60000
s3.request_timeout=10000
# Off = 0,Fatal = 1,Error = 2,Warn = 3,Info = 4,Debug = 5,Trace = 6
s3.loglevel=4
s3.logPrefix=/data/logs/curvefs/aws_ # __CURVEADM_TEMPLATE__ /curvefs/client/logs/aws_ __CURVEADM_TEMPLATE__
s3.async_thread_num=30
# limit all inflight async requests' bytes, |0| means not limited
s3.max_async_request_inflight_bytes=104857600
# throttle
s3.throttle.iopsTotalLimit=0
s3.throttle.iopsReadLimit=0
s3.throttle.iopsWriteLimit=0
s3.throttle.bpsTotalMB=0
s3.throttle.bpsReadMB=0
s3.throttle.bpsWriteMB=0

# TODO(hongsong): limit bytes„ÄÅiops/bps
#### disk cache options
# 0:not enable disk cache 1:onlyread 2:read/write
diskCache.diskCacheType=2  # __ANSIBLE_TEMPLATE__ {{ client_disk_cache_type | default('2') }} __ANSIBLE_TEMPLATE__
# the file system writes files use flush or not
diskCache.forceFlush=true
# the interval of check to trim disk cache
diskCache.trimCheckIntervalSec=5
# the interval of check to trim load file to s3
diskCache.asyncLoadPeriodMs=5
# start trim file when disk cache use ratio is Greater than fullRatio,
# util less than safeRatio
diskCache.fullRatio=90
diskCache.safeRatio=70
# the max size disk cache can use
diskCache.maxUsableSpaceBytes=107374182400
# the max time system command can run
diskCache.cmdTimeoutSec=300
# directory of disk cache
diskCache.cacheDir=/mnt/curvefs_cache  # __CURVEADM_TEMPLATE__ /curvefs/client/data/cache __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ /mnt/curvefs_disk_cache/{{ 99999999 | random | to_uuid | upper }} __ANSIBLE_TEMPLATE__

# the write throttle bps of disk cache, default 80MB/s
diskCache.avgFlushBytes=83886080
# the write burst bps of disk cache, default 100MB/s
diskCache.burstFlushBytes=104857600
# the times that write burst bps can continue, default 180s
diskCache.burstSecs=180
# the write throttle iops of disk cache, default no limit
diskCache.avgFlushIops=0
# the read throttle bps of disk cache, default 80MB/s
diskCache.avgReadFileBytes=83886080
# the read throttle iops of disk cache, default no limit
diskCache.avgReadFileIops=0

#### common
client.common.logDir=/data/logs/curvefs  # __CURVEADM_TEMPLATE__ /curvefs/client/logs __CURVEADM_TEMPLATE__
# we have loglevel: {0,3,6,9}
# as the number increases, it becomes more and more detailed
client.loglevel=0
client.dummyserver.startport=9000
