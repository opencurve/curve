#
# trash option
#
trash.scanPeriodSec=600
trash.expiredAfterSec=604800

# s3
# if s3.enableDeleteObjects set True, batch size limit the object num of delete count per delete request
s3.batchsize=100
# if s3 sdk support batch delete objects, set True; other set False
s3.enableDeleteObjects=False
# http = 0, https = 1
s3.http_scheme=0
s3.verify_SSL=False
s3.max_connections=32
s3.connect_timeout=60000
s3.request_timeout=10000
# Off = 0,Fatal = 1,Error = 2,Warn = 3,Info = 4,Debug = 5,Trace = 6
s3.loglevel=4
s3.logPrefix=/tmp/curvefs/metaserver/aws_
s3.async_thread_num=10
# throttle
s3.throttle.iopsTotalLimit=0
s3.throttle.iopsReadLimit=0
s3.throttle.iopsWriteLimit=0
s3.throttle.bpsTotalMB=0
s3.throttle.bpsReadMB=0
s3.throttle.bpsWriteMB=0
s3.useVirtualAddressing=false
# s3 workqueue
s3compactwq.enable=True
s3compactwq.thread_num=2
s3compactwq.queue_size=5
# fragments threshold in a s3chuninfolist
s3compactwq.fragment_threshold=20
# max chunks to process per compact task
s3compactwq.max_chunks_per_compact=10
# roughly control the compact freq
s3compactwq.enqueue_sleep_ms=1000
s3compactwq.s3infocache_size=100
# workaround read failure when diskcache is enabled
s3compactwq.s3_read_max_retry=5
s3compactwq.s3_read_retry_interval=5 # in seconds

# metaserver listen ip and port
# these two config items ip and port can be replaced by start up options `-ip` and `-port`
global.ip=127.0.0.1  # __CURVEADM_TEMPLATE__ ${service_addr} __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ {{ curvefs_metaserver_listen_host }} __ANSIBLE_TEMPLATE__
global.port=16701  # __CURVEADM_TEMPLATE__ ${service_port} __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ {{ curvefs_metaserver_listen_port }} __ANSIBLE_TEMPLATE__
global.external_ip=127.0.0.1  # __CURVEADM_TEMPLATE__ ${service_external_addr} __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ {{ curvefs_metaserver_listen_host }} __ANSIBLE_TEMPLATE__
global.external_port=16701 # __CURVEADM_TEMPLATE__ ${service_external_port} __CURVEADM_TEMPLATE__
global.enable_external_server=false

# metaserver log directory
# this config item can be replaced by start up option `-log_dir`
metaserver.common.logDir=/tmp/curvefs/metaserver  # __CURVEADM_TEMPLATE__ ${prefix}/logs __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ /tmp/{{ inventory_hostname }}/curvefs/metaserver __ANSIBLE_TEMPLATE__
# we have loglevel: {3,6,9}
# as the number increases, it becomes more and more detailed
metaserver.loglevel=0
# metaserver meta file path, every metaserver need persist MetaServerMetadata on its own disk
metaserver.meta_file_path=./0/metaserver.dat  # __CURVEADM_TEMPLATE__ ${prefix}/data/metaserver.dat __CURVEADM_TEMPLATE__

# copyset data uri
# all uri (data_uri/raft_log_uri/raft_meta_uri/raft_snapshot_uri/trash.uri) are ${protocol}://${path}
# e.g., when save data to local disk, protocol is `local`, path can be `absolute path` or `relative path`
#       local:///mnt/data or local://./data
# this config item can be replaced by start up option `-dataUri`
copyset.data_uri=local://./0/copysets  # __CURVEADM_TEMPLATE__ local://${prefix}/data/copysets __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ local://{{ curvefs_metaserver_data_root }}/copysets __ANSIBLE_TEMPLATE__

# copyset reload concurrency
# when server restart, it will reload copysets from `copyset.data_uri`
# if value set to 1, means all copysets are loaded one by one it may cause a long start-up time
# if value bigger than 1, means at most |load_concurrency| copysets are loaded parallelly
# but larger value may cause higher cpu/memory/disk usgae
copyset.load_concurrency=1

# if the difference between the applied_index of the current replica and the
# committed_index on the leader is less than |finishLoadMargin|, it's
# determined that the copyset has been loaded completed
copyset.finishload_margin=2000

# the maximum number of retries to check whether a copyset is loaded completed
copyset.check_retrytimes=3

# sleep time in microseconds between different cycles check whether copyset is loaded
copyset.check_loadmargin_interval_ms=1000

# raft election timeout in milliseconds
# follower would become a candidate if it doesn't receive any message
# from the leader in |election_timeout_ms| milliseconds
copyset.election_timeout_ms=1000

# raft snapshot interval in seconds
# snapshot saving would be triggered every |snapshot_interval_s| seconds if this was reset as a positive number
# if |snapshot_interval_s| <= 0, the time based snapshot would be disabled
copyset.snapshot_interval_s=1800

# raft catchup margin
# regard a adding peer as caught up if the margin between
# the last_log_index of this peer and the last_log_index of leader is less than |catchup_margin|
copyset.catchup_margin=1000

# raft-log storage uri
# this config item can be replaced by start up option `-raftLogUri`
copyset.raft_log_uri=local://./0/copysets  #  __CURVEADM_TEMPLATE__ local://${prefix}/data/copysets __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ local://{{ curvefs_metaserver_data_root }}/copysets __ANSIBLE_TEMPLATE__

# raft-meta storage uri
# this config item can be replaced by start up option `-raftMetaUri`
copyset.raft_meta_uri=local://./0/copysets  # __CURVEADM_TEMPLATE__ local://${prefix}/data/copysets __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ local://{{ curvefs_metaserver_data_root }}/copysets __ANSIBLE_TEMPLATE__

# raft-snapshot storage uri
# this config item can be replaced by start up option `-raftSnapshotUri`
copyset.raft_snapshot_uri=local://./0/copysets  # __CURVEADM_TEMPLATE__ local://${prefix}/data/copysets __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ local://{{ curvefs_metaserver_data_root }}/copysets __ANSIBLE_TEMPLATE__

# trash-uri
# if coyset was deleted, its data path was first move to trash directory
# this config item can be replaced by start up option `-trashUriUri`
copyset.trash.uri=local://./0/trash  # __CURVEADM_TEMPLATE__ local://${prefix}/data/trash __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ local://{{ curvefs_metaserver_data_root }}/trash __ANSIBLE_TEMPLATE__

# after a copyset data has been move to trash directory for more than #expired_aftersec seconds
# its data will be deleted
copyset.trash.expired_aftersec=300

# backend trash thread scan interval in seconds
copyset.trash.scan_periodsec=120

# number of reqeusts being processed
# this config item should be tuned according cpu/memory/disk
service.max_inflight_request=5000

### apply queue options for each copyset
### apply queue is used to isolate raft threads, each worker has its own queue
### whan a task can be applied it's been pushed into a corresponding worker queue by certain rules
# number of apply queue workers for each, each worker will start a indepent thread
applyqueue.worker_count=1

# apply queue depth for each copyset
# all tasks in queue must be done when do raft snapshot, and raft apply and raft snapshot are executed in same thread
# so, if queue depth is too large, it will cause other tasks to wait too long for apply
applyqueue.queue_depth=1

# number of worker threads that created by brpc::Server
# if set to |auto|, threads create by brpc::Server is equal to `getconf _NPROCESSORS_ONLN` + 1
# if set to a fixed value, it will create |wroker_count| threads, and its range is [4, 1024]
# it is recommended to set it to |auto| unless there is a significant performance improvement
bthread.worker_count=auto

### Braft related flags
### These configurations are ignored if the command line startup options are set
# Call fsync when need
# braft default is True. Setting to false can greatly improve performance.
# We can select according to the specified scene.
braft.raft_sync=True
# Sync log meta, snapshot meta and raft meta
# braft default is False
braft.raft_sync_meta=True
# Call fsync when a segment is closed
# braft default is False
braft.raft_sync_segments=True
# Use fsync rather than fdatasync to flush page cache
# braft default is True
braft.raft_use_fsync_rather_than_fdatasync=False
# Max num of install_snapshot tasks per disk at the same time
# braft default is 1000
braft.raft_max_install_snapshot_tasks_num=10

#
# MDS settings
#
# support multiple addr, use ',' to seperate addr: 127.0.0.1:6700,127.0.0.1:6701
mds.listen.addr=127.0.0.1:6700  # __CURVEADM_TEMPLATE__ ${cluster_mds_addr} __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ {{ groups.mds | join_peer(hostvars, "mds_listen_port") }} __ANSIBLE_TEMPLATE__
# the max retry times for metaserver to register to mds
mds.register_retries=100
# the time of rpc timeout when metaserver register to mds, normally 1000ms
mds.register_timeoutMs=1000
# the interval of metaserver send heartbeat to mds, normally 10s
mds.heartbeat_intervalSec=10
# the rpc timeout of metaserver send heartbeat to mds, normally1000ms
mds.heartbeat_timeoutMs=1000

#
# partition clean settings
#
# partition clean manager scan partition every scanPeriodSec
partition.clean.scanPeriodSec=10
# partition clean manager delete inode every inodeDeletePeriodMs
partition.clean.inodeDeletePeriodMs=500

##### mdsOpt
# RPC total retry time with MDS
mdsOpt.mdsMaxRetryMS=16000
# The maximum timeout of RPC communicating with MDS.
# The timeout of exponential backoff cannot exceed this value
mdsOpt.rpcRetryOpt.maxRPCTimeoutMS=2000
# RPC timeout for once communication with MDS
mdsOpt.rpcRetryOpt.rpcTimeoutMs=500
# RPC with mds needs to sleep for a period of time before each retry
mdsOpt.rpcRetryOpt.rpcRetryIntervalUS=50000
# Switch if the number of consecutive retries on the current MDS exceeds the limit.
# The number of failures includes the number of timeout retries
mdsOpt.rpcRetryOpt.maxFailedTimesBeforeChangeAddr=2
# The normal retry times for trigger wait strategy
mdsOpt.rpcRetryOpt.normalRetryTimesBeforeTriggerWait=3
# Sleep interval for wait
mdsOpt.rpcRetryOpt.waitSleepMs=1000
mdsOpt.rpcRetryOpt.addrs=127.0.0.1:6700,127.0.0.1:6701,127.0.0.1:6702  # __CURVEADM_TEMPLATE__ ${cluster_mds_addr} __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ {{ groups.mds | join_peer(hostvars, "mds_listen_port") }} __ANSIBLE_TEMPLATE__
#
# storage settings
#
# metaserver storage data directory
storage.data_dir=/tmp/storage  # __CURVEADM_TEMPLATE__ ${prefix}/data __CURVEADM_TEMPLATE__
# metaserver max memory quota bytes (default: 30GB)
storage.max_memory_quota_bytes=32212254720
# metaserver max disk quota bytes (default: 2TB)
storage.max_disk_quota_bytes=2199023255552
