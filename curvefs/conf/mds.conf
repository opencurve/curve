#
# mds options
#
mds.listen.addr=127.0.0.1:6700  #__CURVEADM_TEMPLATE__ ${service_addr}:${service_port} __CURVEADM_TEMPLATE__   __ANSIBLE_TEMPLATE__ {{ curvefs_mds_listen_host }}:{{ curvefs_mds_listen_port }} __ANSIBLE_TEMPLATE__
# dummy server port
mds.dummy.port=7700  # __CURVEADM_TEMPLATE__ ${service_dummy_port} __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ {{ curvefs_mds_listen_dummy_port }} __ANSIBLE_TEMPLATE__
mds.common.logDir=/tmp/curvefs/mds  # __CURVEADM_TEMPLATE__ ${prefix}/logs __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ /tmp/{{ inventory_hostname }}/curvefs/mds __ANSIBLE_TEMPLATE__
mds.loglevel=0

#
# space options
#
space.addr=127.0.0.1:19999  # __ANSIBLE_TEMPLATE__ {{ groups.space | join_peer(hostvars, "space_listen_port") }} __ANSIBLE_TEMPLATE__
space.rpcTimeoutMs=500

#
# metaserver options
#
metaserver.addr=127.0.0.1:6701  # __CURVEADM_TEMPLATE__ ${cluster_mds_addr} __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ {{ groups.metaserver | join_peer(hostvars, "metaserver_listen_port") }} __ANSIBLE_TEMPLATE__
metaserver.rpcTimeoutMs=5000
metaserver.rpcRertyTimes=3
metaserver.rpcRetryIntervalUs=1000000

#
# etcd options
#
# etcd listen address
etcd.endpoint=127.0.0.1:2379  # __CURVEADM_TEMPLATE__ ${cluster_etcd_addr} __CURVEADM_TEMPLATE__  __ANSIBLE_TEMPLATE__ {{ groups.etcd | join_peer(hostvars, "etcd_listen_client_port") }} __ANSIBLE_TEMPLATE__
# timeout for establishing a connection
etcd.dailtimeoutMs=5000
# timeout for the operation
etcd.operation.timeoutMs=5000
# number of times a failed operation can be retried
etcd.retry.times=3

#
# leader election options
#
leader.sessionInterSec=5
leader.electionTimeoutMs=0

#
# topology config
#
# time interval flush data to db
mds.topology.TopologyUpdateToRepoSec=60
# max partition number in copyset 2^7
mds.topology.MaxPartitionNumberInCopyset=128
# inode number in each partition 2^20 [0, 2^20-1]
mds.topology.IdNumberInPartition=1048576
# default create partition number 12
mds.topology.CreatePartitionNumber=12
# max copyset num in metaserver
mds.topology.MaxCopysetNumInMetaserver=100
# Topology update metric interval
mds.topology.UpdateMetricIntervalSec=60

#
# heartbeat config
#
# heartbeat interval between metaserver and mds
mds.heartbeat.intervalMs=10000
# heartbeat miss time between metaserver and mds
mds.heartbeat.misstimeoutMs=30000
# heartbeat offline time between metaserver and mds
mds.heartbeat.offlinetimeoutMs=1800000
# After mds is started, a certain time delay starts to guide the metaserver to delete data
# default is 20 min
mds.heartbeat.clean_follower_afterMs=1200000

#
# schedule config
#
# recoverScheduler switch
mds.enable.recover.scheduler=true
# copysetScheduler switch
mds.enable.copyset.scheduler=true
# leaderScheduler switch
mds.enable.leader.scheduler=true
# RecoverScheduler round interval, the unit is second
mds.recover.scheduler.intervalSec=5
# copysetScheduler round interval, the unit is second
mds.copyset.scheduler.intervalSec=5
# leaderScheduler round interval, the unit is second
mds.leader.scheduler.intervalSec=5
# the percentage difference for copysetScheduler to
# determine whether resource balancing is required
# based on the difference in resource usage percent, default is 15%
mds.copyset.scheduler.balanceRatioPercent=15
# Concurrency of operator on each metaserver
mds.schduler.operator.concurrent=1
# transfer leader timeout, after the timeout, mds removes the operator from the memory
mds.schduler.transfer.limitSec=60
# remove a replica timeout, after the timeout, mds removes the operator from the memory
mds.scheduler.remove.limitSec=300
# add a replica timeout, after the timeout, mds removes the operator from the memory
mds.scheduler.add.limitSec=1800
# change the replica timeout, after the timeout, mds removes the operator from the memory
mds.scheduler.change.limitSec=1800
# metaserver can be used as target leader only after starting coolingTimeSec_, the unit is second
mds.scheduler.metaserver.cooling.timeSec=1800

#
# fsmananger config
#
# the backend thread check whether fs is able to delete,
# check partition of deleting fs is deleting
mds.fsmanager.backEndThreadRunInterSec=10
# number of threads that load space info of volume
mds.fsmanager.reloadSpaceConcurrency=10
# the client timeout is 20s default, umount fs if timeout
mds.fsmanager.client.timeoutSec=20

#### s3
# TODO(huyao): use more meaningfull name
# http = 0, https = 1
s3.http_scheme=0
s3.verify_SSL=False
s3.region=us-east-1
s3.maxConnections=32
s3.connectTimeout=60000
s3.requestTimeout=10000
# Off = 0,Fatal = 1,Error = 2,Warn = 3,Info = 4,Debug = 5,Trace = 6
s3.logLevel=4
s3.logPrefix=/data/logs/curvefs/aws_  # __CURVEADM_TEMPLATE__ /curvefs/client/logs/aws_ __CURVEADM_TEMPLATE__
s3.asyncThreadNum=30
# limit all inflight async requests' bytes, |0| means not limited
s3.maxAsyncRequestInflightBytes=104857600
# throttle
s3.throttle.iopsTotalLimit=0
s3.throttle.iopsReadLimit=0
s3.throttle.iopsWriteLimit=0
s3.throttle.bpsTotalMB=0
s3.throttle.bpsReadMB=0
s3.throttle.bpsWriteMB=0
s3.useVirtualAddressing=false

# TTL(millisecond) for distributed lock
dlock.ttl_ms=5000
# lock try timeout(millisecond) for distributed lock
dlock.try_timeout_ms=300
# lock try interval(millisecond) for distributed lock
dlock.try_interval_ms=30

#### Options for interactive with curvebs MDS
# RPC total retry time with MDS
bs.mds.maxRetryMs=8000
# RPC timeout for once communication with MDS
bs.mds.rpcTimeoutMs=500
# The maximum timeout of RPC communicating with MDS.
# The timeout of exponential backoff cannot exceed this value
bs.mds.maxRPCTimeoutMs=2000
# RPC with mds needs to sleep for a period of time before each retry
bs.mds.rpcRetryIntervalUs=50000
# Switch if the number of consecutive retries on the current MDS exceeds the limit.
# The number of failures includes the number of timeout retries
bs.mds.maxFailedTimesBeforeChangeMDS=2
# The normal retry times for trigger wait strategy
bs.mds.normalRetryTimesBeforeTriggerWait=3
# sleep interval in ms for wait
bs.mds.waitSleepMs=1000

#### Options for volume space deallcatable setting
mds.space.calIntervalSec=60
