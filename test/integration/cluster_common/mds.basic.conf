#
#  Copyright (c) 2020 NetEase Inc.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#

#
# Mds service port
#
mds.listen.addr=127.0.0.1:6666

#
# ETCD related configurations
#
# ETCD address
mds.etcd.endpoint=localhost:2221
# The timeout period for establishing a connection with a client
mds.etcd.dailtimeoutMs=5000
# The timeout period for client to perform put/get/txn and other operations
mds.etcd.operation.timeoutMs=5000
# The number of times a client operation failed and can be retried
mds.etcd.retry.times=3

#
# Configuration related to segment allocation statistics
#
# The interval between persisting data in memory to ETCD, in milliseconds
mds.segment.alloc.periodic.persistInterMs=1000
# The retry interval in ms in case of an error
mds.segment.alloc.retryInterMs=1000


# During the leader campaign, a session will be created in seconds, as the value unit for the interface of the go side code is seconds
mds.leader.sessionInterSec=5
# The timeout for leader election. If set to 0, the election will block indefinitely if unsuccessful. If set to a value greater than 0, an error will be returned if not elected as leader within the electionTimeoutMs duration. 
# Here, a timeout of 10 minutes is set, and if it times out, the MDS will continue the election process.
mds.leader.electionTimeoutMs=0

#
# Schedule related configurations
#
# copysetScheduler switch
mds.enable.copyset.scheduler=true
# leaderScheduler switch
mds.enable.leader.scheduler=true
# recoverScheduler switch
mds.enable.recover.scheduler=true
# replicaScheduler switch
mds.enable.replica.scheduler=true
# copysetScheduler round interval, measured in seconds
mds.copyset.scheduler.intervalSec=5
# replicaScheduler round interval, measured in seconds
mds.replica.scheduler.intervalSec=5
# leaderScheduler round interval, measured in seconds
mds.leader.scheduler.intervalSec=30
# recoverScheduler round interval, measured in seconds
mds.recover.scheduler.intervalSec=5
# The concurrency of operators on each disk
mds.schduler.operator.concurrent=4
# The leader changes the timeout time, and after the timeout, the mds removes the operator from memory
mds.schduler.transfer.limitSec=1800
# Reduce the replica timeout by one, and after the timeout, the mds removes the operator from memory
mds.scheduler.remove.limitSec=1800
# Add a replica timeout, after which the mds removes the operator from memory
mds.scheduler.add.limitSec=1800
# The range of copyset quantity cannot exceed the percentage of the mean
mds.scheduler.copysetNumRangePercent=0.05
# The scatter width of the copyset on chunkserver cannot exceed the percentage of the minimum value
mds.schduler.scatterWidthRangePerent=0.2
# There are more than a certain number of chunkserver offline on a server, and no recovery will be performed
mds.chunkserver.failure.tolerance=3
# chunkserver starts coolingTimeSec_ Only then can it be used as a target leader, with the unit of s
# TODO(lixiaocui):  It needs to be related to the time interval of snapshots to some extent.
mds.scheduler.chunkserver.cooling.timeSec=1800

#
# Heartbeat related configuration, in ms
#
# Heartbeat interval between chunkserver and mds
mds.heartbeat.intervalMs=1000
# The time of heartbeat miss between chunkserver and mds
mds.heartbeat.misstimeoutMs=3000
# Mds marked offlinetimeout as offline after heartbeat miss
mds.heartbeat.offlinetimeoutMs=1800000
# After starting the mds, delay for a certain period of time to guide chunkserver in deleting physical data
# The reason for delayed deletion is noted in the code
mds.heartbeat.clean_follower_afterMs=1200000

#
# namespace cache related
#
# The cache size of namestorage, where 0 indicates no caching
# Based on a minimum space budget of 10GB per file. Including oversold (2x)
# Number of files = 5PB/10GB ~= 524288 files
# sizeof(namespace object) * 524288 ~= 89Byte * 524288 ~= 44MB space
# 16MB chunk size, 1 segment 1GB
# sizeof(segment object) * 2621440 ~= (32+(1024/16) * 12) * 2621440 ~= 1.95 GB
# Data volume: about 3GB
# Record quantity: 524288+2621440 ~= about 300w
mds.cache.count=100000

#
# mysql Database config
#
# The database name used by the database
mds.DbName=cluster_common_curve_mds
# Database username
mds.DbUser=root
# Database address
mds.DbUrl=localhost
# Database login password
mds.DbPassword=qwer
mds.DbPoolSize=128

#
# mds.session settings
#
# mds.session expiration time, in us
mds.session.leaseTimeUs=5000000
# Tolerable time of clock asynchrony between client and mds, in units of us
mds.session.toleranceTimeUs=500000
# mds.session Background Scan Thread Scan Interval Time, Unit: us
mds.session.intevalTimeUs=500000

#
# auth settings
#
# root User Password
mds.auth.rootPassword=root_password

#
# file lock setting
#
# File lock bucket size for mds
mds.filelock.bucketNum=8

#
# topology config
#
# The time interval for Toplogy to periodically refresh into the database
mds.topology.TopologyUpdateToRepoSec=60
# Request timeout for creating all copysets on chunkserver
mds.topology.CreateCopysetRpcTimeoutMs=10000
# Request to create copyset on chunkserver retry count
mds.topology.CreateCopysetRpcRetryTimes=20
# Request to create copyset on chunkserver retry interval
mds.topology.CreateCopysetRpcRetrySleepTimeMs=1000
# Topology module refresh metric interval
mds.topology.UpdateMetricIntervalSec=1
# The percentage of physical pool usage, even if the usage exceeds this value, it will no longer be allocated to this pool
mds.topology.PoolUsagePercentLimit=90
# Multi pool selection pool strategy 0:Random, 1:Weight
mds.topology.choosePoolPolicy=0

#
# copyset config
# Default value, not enabled when 0
#
# Generate copyset retry count
mds.copyset.copysetRetryTimes=10
# The maximum variance that the scatterWidth of all chunkservers must meet
mds.copyset.scatterWidthVariance=0
# The maximum standard deviation that the scatterWidth of all chunkservers must meet
mds.copyset.scatterWidthStandardDevation=0
# The maximum range that the scatterWidth of all chunkservers needs to meet
mds.copyset.scatterWidthRange=0
# Percentage of Deviation from the Mean ScatterWidth of All Chunk Servers. Setting a high percentage for scatterWidth deviation can lead to some machines having
# excessively small scatterWidth, which impacts machine recovery times and reduces the overall reliability of the cluster. Additionally, it can result in certain machines
# having excessively large scatterWidth values, causing copysets on these chunk servers to be scattered across various machines. When other machines write data, these servers
# with larger scatterWidth can become performance bottlenecks.
# Conversely, setting a low percentage for scatterWidth deviation requires a higher degree of scatterWidth uniformity, demanding more from the copyset algorithm. This
# can lead to the algorithm being unable to produce optimal results. It is recommended to set the value at 20 for a balance between these factors.
mds.copyset.scatterWidthFloatingPercentage=20

#
# curvefs config
#
# The default chunk size for curvefs is 16MB = 16*1024*1024 = 16777216
mds.curvefs.defaultChunkSize=16777216

#
# chunkseverclient config
#
# RPC timeout
mds.chunkserverclient.rpcTimeoutMs=500
# RPC retry count
mds.chunkserverclient.rpcRetryTimes=5
# RPC retry interval
mds.chunkserverclient.rpcRetryIntervalMs=500
# The maximum number of retries from each chunkserver getleader in the copyset
mds.chunkserverclient.updateLeaderRetryTimes=5
# The interval between each round of each chunkserver getleader in the copyset must be greater than the time for selecting the master in the raft
mds.chunkserverclient.updateLeaderRetryIntervalMs=5000

#
# common options
#
# Log storage folder
mds.common.logDir=./runlog/

