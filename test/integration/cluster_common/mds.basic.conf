#
#  Copyright (c) 2020 NetEase Inc.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#

#
#Mds service port
#
mds.listen.addr=127.0.0.1:6666

#
#ETCD related configurations
#
#ETCD address
mds.etcd.endpoint=localhost:2221
#The timeout period for establishing a connection with a client
mds.etcd.dailtimeoutMs=5000
#The timeout period for client to perform put/get/txn and other operations
mds.etcd.operation.timeoutMs=5000
#The number of times a client operation failed and can be retried
mds.etcd.retry.times=3

#
#Configuration related to segment allocation statistics
#
#The interval between persisting data in memory to ETCD, in milliseconds
mds.segment.alloc.periodic.persistInterMs=1000
#The retry interval in ms in case of an error
mds.segment.alloc.retryInterMs=1000


#During the leader campaign, a session will be created in seconds, as the value unit for the interface of the go side code is seconds
mds.leader.sessionInterSec=5
#The timeout period for the leader election. If it is 0 and the election is unsuccessful, it will continue to block. If it is greater than 0, it will be in the selectionTimeoutMs time
#If a leader is not selected, an error will be returned. Set a 10 minute timeout here, after which MDS will continue to run for election
mds.leader.electionTimeoutMs=0

#
#Schedule related configurations
#
#CopysetScheduler switch
mds.enable.copyset.scheduler=true
#LeaderScheduler switch
mds.enable.leader.scheduler=true
#RecoverScheduler switch
mds.enable.recover.scheduler=true
#ReplicaScheduler switch
mds.enable.replica.scheduler=true
#CopysetScheduler round interval, in seconds
mds.copyset.scheduler.intervalSec=5
#The interval between replicaScheduler rounds, in seconds
mds.replica.scheduler.intervalSec=5
#The interval between leaderScheduler rounds, in seconds
mds.leader.scheduler.intervalSec=30
#The interval between recoverScheduler rounds, in seconds
mds.recover.scheduler.intervalSec=5
#The concurrency of operators on each disk
mds.schduler.operator.concurrent=4
#The leader changes the timeout time, and after the timeout, the mds removes the operator from memory
mds.schduler.transfer.limitSec=1800
#Reduce the replica timeout by one, and after the timeout, the mds removes the operator from memory
mds.scheduler.remove.limitSec=1800
#Add a replica timeout, after which the mds removes the operator from memory
mds.scheduler.add.limitSec=1800
#The range of copyset quantity cannot exceed the percentage of the mean
mds.scheduler.copysetNumRangePercent=0.05
#The scatter width of the copyset on chunkserver cannot exceed the percentage of the minimum value
mds.schduler.scatterWidthRangePerent=0.2
#There are more than a certain number of chunkservers offline on a server, and no recovery will be performed
mds.chunkserver.failure.tolerance=3
#Chunkserver starts coolingTimeSec_ Only then can it be used as a target leader, with the unit of s
#TODO (Lixiaocui): Continuation is to some extent related to the time interval of the snapshot
mds.scheduler.chunkserver.cooling.timeSec=1800

#
#Heartbeat related configuration, in ms
#
#Heartbeat interval between chunkserver and mds
mds.heartbeat.intervalMs=1000
#The time of heartbeat miss between chunkserver and mds
mds.heartbeat.misstimeoutMs=3000
#Mds marked offline timeout as offline after heartbeat miss
mds.heartbeat.offlinetimeoutMs=1800000
#After starting the mds, delay for a certain period of time to guide chunkserver in deleting physical data
#The reason for delayed deletion is noted in the code
mds.heartbeat.clean_follower_afterMs=1200000

#
#Namespace cache related
#
#The cache size of namestorage, where 0 indicates no caching
#Based on a minimum space budget of 10GB per file. Including oversold (2x)
#Number of files=5PB/10GB~=524288 files
#Sizeof (namespace object) * 524288~=89Byte * 524288~=44MB space
#16MB chunk size, 1 segment 1GB
#Sizeof (segment object) * 2621440~=(32+(1024/16) * 12) * 2621440~=1.95 GB
#Data volume: about 3GB
#Record quantity: 524288+2621440~=about 300w
mds.cache.count=100000

#
# mysql Database config
#
#The database name used by the database
mds.DbName=cluster_common_curve_mds
#Database username
mds.DbUser=root
#Database address
mds.DbUrl=localhost
#Database login password
mds.DbPassword=qwer
mds.DbPoolSize=128

#
# mds.session settings
#
#Mds. session expiration time, in us
mds.session.leaseTimeUs=5000000
#Tolerable time of clock asynchrony between client and mds, in units of us
mds.session.toleranceTimeUs=500000
#MDS. Session Background Scan Thread Scan Interval Time, Unit: us
mds.session.intevalTimeUs=500000

#
# auth settings
#
#Root User Password
mds.auth.rootPassword=root_password

#
# file lock setting
#
#File lock bucket size for mds
mds.filelock.bucketNum=8

#
# topology config
#
#The time interval for Toplogy to periodically refresh into the database
mds.topology.TopologyUpdateToRepoSec=60
#Request timeout for creating all copysets on chunkserver
mds.topology.CreateCopysetRpcTimeoutMs=10000
#Request to create copyset on chunkserver retry count
mds.topology.CreateCopysetRpcRetryTimes=20
#Request to create copyset on chunkserver retry interval
mds.topology.CreateCopysetRpcRetrySleepTimeMs=1000
#Topology module refresh metric interval
mds.topology.UpdateMetricIntervalSec=1
#The percentage of physical pool usage, even if the usage exceeds this value, it will no longer be allocated to this pool
mds.topology.PoolUsagePercentLimit=90
#Multi pool selection pool strategy 0: Random, 1: Weight
mds.topology.choosePoolPolicy=0

#
# copyset config
#Default value, not enabled when 0
#
#Generate copyset retry count
mds.copyset.copysetRetryTimes=10
#The maximum variance that the scatterWidth of all chunkservers must meet
mds.copyset.scatterWidthVariance=0
#The maximum standard deviation that the scatterWidth of all chunkservers must meet
mds.copyset.scatterWidthStandardDevation=0
#The maximum range that the scatterWidth of all chunkservers needs to meet
mds.copyset.scatterWidthRange=0
#The percentage of scatterWidth deviation from the mean for all chunkservers
#The offset percentage of scatterwidth is set too large, causing some machines to have too small scatterwidth, which affects machine recovery time and recovery
#Time can lead to a decrease in the reliability of the cluster; Secondly, it can cause some machines to have too large scatterwith, and some chunkservers may have
#Copysets are scattered on various machines, and once written to by other machines, these larger machines become hotspots
#The offset percentage of scatterwidth is set too small, resulting in a higher requirement for the average degree of scatterwidth and a higher requirement for the copyset algorithm,
#As a result, the algorithm may not produce ideal results, it is recommended to set the value to 20
mds.copyset.scatterWidthFloatingPercentage=20

#
# curvefs config
#
#The default chunk size for curvefs is 16MB=16 * 1024 * 1024=16777216
mds.curvefs.defaultChunkSize=16777216

#
# chunkseverclient config
#
#RPC timeout
mds.chunkserverclient.rpcTimeoutMs=500
#RPC retry count
mds.chunkserverclient.rpcRetryTimes=5
#RPC retry interval
mds.chunkserverclient.rpcRetryIntervalMs=500
#The maximum number of retries from each chunkserver getleader in the copyset
mds.chunkserverclient.updateLeaderRetryTimes=5
#The interval between each round of each chunkserver getleader in the copyset must be greater than the time for selecting the master in the raft
mds.chunkserverclient.updateLeaderRetryIntervalMs=5000

#
# common options
#
#Log storage folder
mds.common.logDir=./runlog/

