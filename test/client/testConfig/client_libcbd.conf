#
# mds一侧配置信息
#
# mds的地址信息
mds.listen.addr=127.0.0.1:9151
# 与mds通信的rpc超时时间
chunkserver.rpcTimeoutMS=1000
# 与MDS一侧保持一个lease时间内多少次续约
mds.refreshTimesPerLease=4
# 同步调用接口rpc超时时间，Open、GetFileInfo等接口
mds.rpcTimeoutMS=1000
# 与mds通信最大的超时时间
mds.maxRPCTimeoutMS=2000
mds.maxFailedTimesBeforeChangeMDS=10

#
# metacache配置信息
#
# 获取leader的rpc超时时间
metacache.getLeaderTimeOutMS=1000
# 获取leader的重试次数
metacache.getLeaderRetry=3
# 每次重试之前需要先睡眠一段时间
mds.rpcRetryIntervalUS=500

#
# request调度层的配置信息
#
#
# 调度层队列大小，每个文件对应一个队列
# 调度队列的深度会影响client端整体吞吐，这个队列存放的是异步IO任务。
# 队列深度与maxInFlightRPCNum数量有关系，其深度应该大于等于maxInFlightRPCNum
# 因为如果小于maxInFlightRPCNum，client端的整体pipeline效果就会受到影响。
schedule.queueCapacity=4096
# 队列的执行线程数量
# 执行线程所要做的事情就是将IO取出，然后发到网络就返回取下一个网络任务。一个任务从
# 队列取出到发送完rpc请求大概在(20us-100us)，20us是正常情况下不需要获取leader的时候
# 如果在发送的时候需要获取leader，时间会在100us左右，一个线程的吞吐在10w-50w
# 性能已经满足需求
schedule.threadpoolSize=2

#
# 为隔离qemu侧线程引入的任务队列，因为qemu一侧只有一个IO线程
# 当qemu一侧调用aio接口的时候直接将调用push到任务队列就返回，
# 这样libcurve不占用qemu的线程，不阻塞其异步调用
#
# 任务队列深度，这个深度至少要大于maxInFlightRPCNum数量才能保证不阻塞用户IO
isolation.taskQueueCapacity=500000
#
# 任务队列线程池大小, 默认值为1个线程
#
isolation.taskThreadPoolSize=1


#
# io发送相关配置
#
# 失败的OP之间重试睡眠
chunkserver.opRetryIntervalUS=50000
# 失败的OP重试次数
chunkserver.opMaxRetry=3
# 开启基于appliedindex的读，用于性能优化
chunkserver.enableAppliedIndexRead=1
# 下发IO最大的分片KB
global.fileIOSplitMaxSizeKB=64
# libcurve允许的最大未返回的IO数量，每个文件单独一份
# maxInFlightRPCNum用于控制当前client端允许的未返回的IO数量
# brpc一条IO链路上允许最大的未发送的缓冲区大小是8M(默认值)，已每个IO 4K大小
# 那么一条链路上最大允许2K个rpc。(这是极端值状况，因为一个文件的IO基本上都会分发到不同的
# chunkserver rpc链路)，所以这里设置2K应该足够了
global.fileMaxInFlightRPCNum=2048
# 同一个chunkserver连续超时上限次数，如果超过这个值，就会设置为unstable状态
chunkserver.maxStableTimeoutTimes=10
chunkserver.maxRetrySleepIntervalUS=8000000
chunkserver.maxRPCTimeoutMS=8000
chunkserver.maxRetryTimesBeforeConsiderSuspend=20
metacache.rpcRetryIntervalUS=100000
#
# log相关配置
#
# log等级 INFO=0/WARNING=1/ERROR=2/FATAL=3
global.logLevel=0
# 设置log的路径
global.logPath=./runlog/

#
# 初始化阶段向mds注册开关，默认为关
#
mds.registerToMDS=false
