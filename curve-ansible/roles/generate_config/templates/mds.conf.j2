#
# Mds service port
#

mds.listen.addr={{ ansible_ssh_host }}:{{ mds_port }}
mds.dummy.listen.port={{ mds_dummy_port }}
global.subnet={{ mds_subnet }}
global.port={{ mds_port }}

#
# ETCD related configurations
#
# ETCD address
{% set etcd_address=[] -%}
{% for host in groups.etcd -%}
  {% set etcd_ip = hostvars[host].ansible_ssh_host -%}
  {% set etcd_port = hostvars[host].etcd_listen_client_port -%}
  {% set _ = etcd_address.append("%s:%s" % (etcd_ip, etcd_port)) -%}
{% endfor -%}

mds.etcd.endpoint={{ etcd_address | join(',') }}
# The timeout period for establishing a connection with a client
mds.etcd.dailtimeoutMs={{ mds_etcd_dailtimeout_ms }}
# The timeout period for client to perform put/get/txn and other operations
mds.etcd.operation.timeoutMs={{ mds_etcd_operation_timeout_ms }}
# The number of times a client operation failed and can be retried
mds.etcd.retry.times={{ mds_etcd_retry_times }}
# wait dlock timeout
mds.etcd.dlock.timeoutMs={{ mds_etcd_dlock_timeout_ms }}
# dlock lease timeout
mds.etcd.dlock.ttlSec={{ mds_etcd_dlock_ttl_sec }}

#
# Configuration related to segment allocation statistics
#
# The interval between persisting data in memory to ETCD, in milliseconds
mds.segment.alloc.periodic.persistInterMs={{ mds_segment_alloc_periodic_persist_inter_ms }}
# The retry interval in ms in case of an error
mds.segment.alloc.retryInterMs={{ mds_segment_alloc_retry_inter_ms }}

mds.segment.discard.scanIntevalMs={{ mds_segment_discard_scan_interval_ms }}


# During the leader election, a session is created in seconds (the unit of the value for the interface of the go code is s)
# This value is related to the ETCD cluster selection timeout
# The server side of ETCD limits this value to a minimum of 1.5 * election timeout
# Suggest setting the ETCD cluster selection timeout to 1 second
mds.leader.sessionInterSec={{ mds_leader_session_inter_sec }}
# The timeout period for the leader election. If it is 0 and the election is unsuccessful, it will continue to block. If it is greater than 0, it will be in the selectionTimeoutMs time
# If a leader is not selected, an error will be returned
mds.leader.electionTimeoutMs={{ mds_leader_election_timeout_ms }}

#
# Schedule related configurations
#
# copysetScheduler switch
mds.enable.copyset.scheduler={{ mds_enable_copyset_scheduler }}
# leaderScheduler switch
mds.enable.leader.scheduler={{ mds_enable_leader_scheduler }}
# recoverScheduler switch
mds.enable.recover.scheduler={{ mds_enable_recover_scheduler }}
# replicaScheduler switch
mds.enable.replica.scheduler={{ mds_enable_replica_scheduler }}
# Scan scheduler switch
mds.enable.scan.scheduler={{ mds_enable_scan_scheduler }}
# copysetScheduler round interval, measured in seconds
mds.copyset.scheduler.intervalSec={{ mds_copyset_scheduler_interval_sec }}
# replicaScheduler round interval, measured in seconds
mds.replica.scheduler.intervalSec={{ mds_replica_scheduler_interval_sec }}
# leaderScheduler round interval, measured in seconds
mds.leader.scheduler.intervalSec={{ mds_leader_scheduler_interval_sec }}
# recoverScheduler round interval, measured in seconds
mds.recover.scheduler.intervalSec={{ mds_recover_scheduler_interval_sec }}
# Scan scheduler run interval (seconds)
mds.scan.scheduler.intervalSec={{ mds_scan_scheduler_interval_sec }}
# The concurrency of operators on each disk
mds.schduler.operator.concurrent={{ mds_schduler_operator_concurrent }}
# The leader changes the timeout time, and after the timeout, the mds removes the operator from memory
mds.schduler.transfer.limitSec={{ mds_schduler_transfer_limit_sec }}
# Reduce the replica timeout by one, and after the timeout, the mds removes the operator from memory
mds.scheduler.remove.limitSec={{ mds_scheduler_remove_limit_sec }}
# Add a replica timeout, after which the mds removes the operator from memory
mds.scheduler.add.limitSec={{ mds_scheduler_add_limit_sec }}
# change a replica timeout, after which the mds removes the operator from memory
mds.scheduler.change.limitSec={{ mds_scheduler_change_limit_sec }}
# Scan operator timeout (seconds)
mds.scheduler.scan.limitSec={{ mds_scheduler_scan_limit_sec }}
# The range of copyset quantity cannot exceed the percentage of the mean
mds.scheduler.copysetNumRangePercent={{ mds_scheduler_copyset_mum_range_percent }}
# The scatter width of the copyset on chunkserver cannot exceed the percentage of the minimum value
mds.schduler.scatterWidthRangePerent={{ mds_schduler_scatterwidth_range_percent }}
# There are more than a certain number of chunkservers offline on a server, and no recovery will be performed
mds.chunkserver.failure.tolerance={{ mds_chunkserver_failure_tolerance }}
# chunkserver starts coolingTimeSec_ Only then can it be used as a target leader, with the unit of s
# TODO(lixiaocui): Continuation is to some extent related to the time interval of the snapshot
mds.scheduler.chunkserver.cooling.timeSec={{ mds_scheduler_chunkserver_cooling_time_sec }}
# ScanScheduler: scan start hour in one day ([0-23])
mds.scheduler.scan.startHour={{ mds_scheduler_scan_start_hour }}
# ScanScheduler: scan end hour in one day ([0-23])
mds.scheduler.scan.endHour={{ mds_scheduler_scan_end_hour }}
# ScanScheduler: scan interval for the same copyset (seconds)
mds.scheduler.scan.intervalSec={{ mds_scheduler_scan_interval_sec }}
# ScanScheduler: maximum number of scan copysets at the same time for every logical pool
mds.scheduler.scan.concurrent.per.pool={{ mds_scheduler_scan_concurrent_per_pool }}
# ScanScheduler: maximum number of scan copysets at the same time for every chunkserver
mds.scheduler.scan.concurrent.per.chunkserver={{ mds_scheduler_scan_concurrent_per_chunkserver }}

#
# Heartbeat related configuration, in ms
#
# Heartbeat interval between chunkserver and mds
mds.heartbeat.intervalMs={{ mds_heartbeat_interval_ms }}
# The time of heartbeat miss between chunkserver and mds
mds.heartbeat.misstimeoutMs={{ mds_heartbeat_misstimeout_ms }}
# Mds marked offlinetimeout as offline after heartbeat miss
mds.heartbeat.offlinetimeoutMs={{ mds_heartbeat_offlinet_imeout_ms }}
# After starting the mds, delay for a certain period of time to guide chunkserver in deleting physical data
# The reason for delayed deletion is noted in the code
mds.heartbeat.clean_follower_afterMs={{ mds_heartbeat_clean_follower_after_ms }}

#
#Namespace cache related
#
# The cache size of namestorage, where 0 indicates no caching
# Based on a minimum space budget of 10GB per file. Including oversold (2x)
# Number of files = 5PB/10GB ~= 524288 files
# sizeof(namespace object) * 524288 ~= 89Byte * 524288 ~= 44MB space
# 16MB chunk size, 1 segment 1GB
# sizeof(segment object) * 2621440 ~= (32+(1024/16) * 12) * 2621440 ~= 1.95 GB
# Data volume: about 3GB
# Record quantity: 524288+2621440 ~= about 300w
mds.cache.count={{ mds_cache_count }}

#
# mds file record settings
#
# Mds file records expiration time, in units of us
mds.file.expiredTimeUs={{ file_expired_time_us }}
# MDS backend scanning thread scanning file record interval time, unit: us
mds.file.scanIntevalTimeUs={{ mds_file_scan_inteval_time_us }}

#
# auth settings
#
# Root User Password
mds.auth.rootUserName={{ curve_root_username }}
mds.auth.rootPassword={{ curve_root_password }}

#
# file lock setting
#
# File lock bucket size for mds
mds.filelock.bucketNum={{ mds_filelock_bucket_num }}

#
# topology config
#
# The time interval for Toplogy to periodically refresh into the database
mds.topology.TopologyUpdateToRepoSec={{ mds_topology_topology_update_to_repo_sec }}
# Request timeout for creating all copysets on chunkserver
mds.topology.CreateCopysetRpcTimeoutMs={{ mds_topology_create_copyset_rpc_timeout_ms }}
# Request to create copyset on chunkserver retry count
mds.topology.CreateCopysetRpcRetryTimes={{ mds_topology_create_copyset_rpc_retry_times }}
# Request to create copyset on chunkserver retry interval
mds.topology.CreateCopysetRpcRetrySleepTimeMs={{ mds_topology_create_copyset_rpc_retry_sleep_time_ms }}
# Topology module refresh metric interval
mds.topology.UpdateMetricIntervalSec={{ mds_topology_update_metric_interval_sec }}
# The percentage of physical pool usage, even if the usage exceeds this value, it will no longer be allocated to this pool
mds.topology.PoolUsagePercentLimit={{ mds_topology_pool_usage_percent_limit }}
# Multi pool selection pool strategy 0: Random, 1: Weight
mds.topology.choosePoolPolicy={{ mds_topology_choose_pool_policy }}
# enable LogicalPool ALLOW/DENY status
mds.topology.enableLogicalPoolStatus={{ mds_topology_enable_logicalpool_status}}

#
# copyset config
# Default value, not enabled when 0
#
# Generate copyset retry count
mds.copyset.copysetRetryTimes={{ mds_copyset_copyset_retry_times }}
# The maximum variance that the scatterWidth of all chunkservers must meet
mds.copyset.scatterWidthVariance={{ mds_copyset_scatterwidth_variance }}
# The maximum standard deviation that the scatterWidth of all chunkservers must meet
mds.copyset.scatterWidthStandardDevation={{ mds_copyset_scatterwidth_standard_devation }}
# The maximum range that the scatterWidth of all chunkservers needs to meet
mds.copyset.scatterWidthRange={{ mds_copyset_scatterwidth_range }}
# The percentage of deviation from the mean scatterWidth of all chunk servers. Setting a too large percentage of scatterWidth deviation can result in some machines having
# excessively small scatterWidth, affecting machine recovery times. Prolonged recovery times can reduce the cluster's reliability. Additionally, it can lead to some machines having
# excessively large scatterWidth, causing certain chunk servers to scatter copysets across various machines. When other machines write data, these machines with larger scatterWidth
# can become hotspots. Setting a too small percentage of scatterWidth deviation requires a greater average scatterWidth,
# which demands higher copyset algorithm requirements. This can lead to the algorithm being unable
# to produce ideal results. It is recommended to set the value to 20.
mds.copyset.scatterWidthFloatingPercentage={{ mds_copyset_scatterwidth_floating_percentage }}

#
# curvefs config
#
# The default chunk size for curvefs is 16MB = 16*1024*1024 = 16777216
mds.curvefs.defaultChunkSize={{ chunk_size }}
# The default segment size for curves is 1GB = 1*1024*1024*1024 = 1073741824
mds.curvefs.defaultSegmentSize={{ segment_size }}
# The default minimum file size for curvefs is 10GB = 10*1024*1024*1024 = 10737418240
mds.curvefs.minFileLength={{ min_file_length }}
# The default maximum file size for curvefs is 20TB = 20*1024*1024*1024*1024 = 21990232555520
mds.curvefs.maxFileLength={{ max_file_length }}

#
# chunkseverclient config
#
# RPC timeout
mds.chunkserverclient.rpcTimeoutMs={{ mds_chunkserverclient_rpc_timeout_ms }}
# RPC retry count
mds.chunkserverclient.rpcRetryTimes={{ mds_chunkserverclient_rpc_retry_times }}
# RPC retry interval
mds.chunkserverclient.rpcRetryIntervalMs={{ mds_chunkserverclient_rpc_retry_interval_ms }}
# The maximum number of retries from each chunkserver getleader in the copyset
mds.chunkserverclient.updateLeaderRetryTimes={{ mds_chunkserverclient_update_leader_retry_times }}
# The interval between each round of each chunkserver getleader in the copyset must be greater than the time for selecting the master in the raft
mds.chunkserverclient.updateLeaderRetryIntervalMs={{ mds_chunkserverclient_update_leader_retry_interval_ms }}

# snapshotclone config
#
# snapshot clone server address
mds.snapshotcloneclient.addr={{ snapshot_nginx_vip }}:{{ nginx_docker_external_port }}

#
# common options
#
# Log storage folder
mds.common.logDir={{ mds_common_log_dir }}
# In the case of unit testing
# mds.common.logDir=./runlog/

#
#### throttle options ####
#
# iops
mds.throttle.iopsMin={{ throttle_iops_min }}
mds.throttle.iopsMax={{ throttle_iops_max }}
mds.throttle.iopsPerGB={{ throttle_iops_per_GB }}
# bps
mds.throttle.bpsMinInMB={{ throttle_bps_min_in_MB }}
mds.throttle.bpsMaxInMB={{ throttle_bps_max_in_MB }}
mds.throttle.bpsPerGBInMB={{ throttle_bps_per_GB_in_MB }}
