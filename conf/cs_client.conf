#
################### mds side configuration information ##################
#

# Address information for mds, separated by commas for mds clusters
mds.listen.addr=127.0.0.1:6666  # __CURVEADM_TEMPLATE__ ${cluster_mds_addr} __CURVEADM_TEMPLATE__

# Register switch with mds during initialization phase, default to on
mds.registerToMDS=false

# RPC timeout for communication with mds
mds.rpcTimeoutMS=500

# The maximum timeout for rpc communication with mds, and the timeout for exponential backoff cannot exceed this value
mds.maxRPCTimeoutMS=2000

# Total retry time for communication with mds
mds.maxRetryMS=8000

# Switch if the number of consecutive retries on the current mds exceeds this limit, which includes the number of timeout retries
mds.maxFailedTimesBeforeChangeMDS=2

# How many renewals are there within a lease period with MDS
mds.refreshTimesPerLease=4

# The mds RPC interface requires a period of sleep before each retry
mds.rpcRetryIntervalUS=100000

# The normal retry times for trigger wait strategy
mds.normalRetryTimesBeforeTriggerWait=3

# Max retry time for IO-Path request
mds.maxRetryMsInIOPath=86400000

# Sleep interval for wait
mds.waitSleepMs=10000

#
################# metacache Configuration Information ################
#

# Obtain the rpc timeout of the leader
metacache.getLeaderTimeOutMS=500

# Obtain the backup request timeout for the leader
metacache.getLeaderBackupRequestMS=100

# The load balancer method used by getleaer backup request
metacache.getLeaderBackupRequestLbName=rr

# Retrieve the number of retries for the leader
metacache.getLeaderRetry=5

# Obtaining the leader interface requires a period of sleep before each retry
metacache.rpcRetryIntervalUS=100000

#
###############Configuration information of the scheduling layer#############
#

# Scheduling layer queue size, with one queue for each file
# The depth of the scheduling queue can affect the overall throughput of the client, as it stores asynchronous IO tasks..
schedule.queueCapacity=1000000

# Number of execution threads for the queue
# The task of execution threads is to fetch IO and then send it over the network before moving on to the next network task. 
# The time taken for a task, from retrieval from the queue to sending the RPC request, is typically between 20 microseconds to 100 microseconds. 20 microseconds is the normal case when leader acquisition is not needed during the send operation. 
# If leader acquisition is required during sending, the time can be around 100 microseconds. The throughput of one thread ranges from 100,000 to 500,000 operations per second. 
# The performance meets the requirements.
schedule.threadpoolSize=1

# To isolate the task queue introduced by the QEMU side thread, as there is only one IO thread on the QEMU side
# When the QEMU side calls the AIO interface, it directly pushes the call to the task queue and returns,
# This way, libcurve does not occupy QEMU's threads and does not block its asynchronous calls
isolation.taskQueueCapacity=1000000

# The size of the task queue thread pool for isolating QEMU threads, with a default value of 1 thread
isolation.taskThreadPoolSize=1


#
################Configuration related to communication with chunkserver#############
#
# Retrying sleep between OPs with failed read/write interfaces
chunkserver.opRetryIntervalUS=100000

#Number of failed OP retries
chunkserver.opMaxRetry=3

# RPC timeout for communication with chunkserver
chunkserver.rpcTimeoutMS=1000

# Maximum sleep time between retry requests
# Because when the network is congested or the chunkserver is overloaded, it is necessary to increase sleep time
# The maximum time for this is maxRetrySleepIntervalUs
chunkserver.maxRetrySleepIntervalUS=8000000

# The maximum timeout rpc time for retry requests, which follows an exponential backoff strategy
# Because timeout occurs when the network is congested, it is necessary to increase the RPC timeout time
# The maximum time for this is maxTimeoutMS
chunkserver.maxRPCTimeoutMS=8000

# Maximum number of consecutive timeouts for the same chunkserver
# If this value is exceeded, a health check will be conducted, and if the health check fails, it will be marked as unstable
chunkserver.maxStableTimeoutTimes=64
# The timeout of health check requests after consecutive RPC timeouts on chunkserver
chunkserver.checkHealthTimeoutMs=100
# After the number of unstable chunkservers on the same server exceeds this value
# All chunkservers will be marked as unstable
chunkserver.serverStableThreshold=3

# When the underlying chunkserver is under high pressure, unstable may also be triggered
# Due to copyset leader may change, the request timeout time will be set to the default value, resulting in IO hang
# In the case of real downtime, the request will be processed after a certain number of retries
# If you keep trying again, it's not a downtime situation, and at this point, the timeout still needs to enter the exponential backoff logic
# When the number of retries for a request exceeds this value, its timeout must enter exponential backoff
chunkserver.minRetryTimesForceTimeoutBackoff=5

# When an RPC retry exceeds maxRetryTimesBeforeConsiderSuspend
# Record as suspended IO, metric will alarm
chunkserver.maxRetryTimesBeforeConsiderSuspend=20

#
#################File level configuration items#############
#
# Libcurve allows for the maximum number of unreturned rpcs in the underlying rpc scheduling, with each file's inflight RPC being independent
global.fileMaxInFlightRPCNum=64

# The maximum sharding KB for file IO distribution to the underlying chunkserver
global.fileIOSplitMaxSizeKB=64

#
################# Log related configuration###############
#
# log level INFO=0/WARNING=1/ERROR=2/FATAL=3
global.logLevel=0
# Set the path of the log
global.logPath=/data/log/curve/  # __CURVEADM_TEMPLATE__ ${prefix}/logs __CURVEADM_TEMPLATE__
# In the case of unit testing
# logpath=./runlog/

#
############### metric configuration information #############
#
global.metricDummyServerStartPort=9000

#
# session map file, storing the mapping from filename to path of the opened file
#
global.sessionMapPath=./session_map.json

##### discard configurations #####
# enable/disable discard
discard.enable=false
# discard granularity
discard.granularity=4096
# discard cleanup task delay times in millisecond
discard.taskDelayMs=60000
