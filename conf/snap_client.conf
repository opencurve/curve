#
###################MDS side configuration information##################
#

# Address information for mds, separated by commas for mds clusters
mds.listen.addr=127.0.0.1:6666  # __CURVEADM_TEMPLATE__ ${cluster_mds_addr} __CURVEADM_TEMPLATE__

# Register switch with mds during initialization phase, default to on
mds.registerToMDS=false

# RPC timeout for communication with mds
mds.rpcTimeoutMS=500

# The maximum timeout for rpc communication with mds, and the timeout for exponential backoff cannot exceed this value
mds.maxRPCTimeoutMS=2000

# Total retry time for communication with mds
mds.maxRetryMS=8000

# Switch if the number of consecutive retries on the current mds exceeds this limit, which includes the number of timeout retries
mds.maxFailedTimesBeforeChangeMDS=2

# How many renewals are there within a lease period with MDS
mds.refreshTimesPerLease=4

# The mds RPC interface requires a period of sleep before each retry
mds.rpcRetryIntervalUS=100000

# The normal retry times for trigger wait strategy
mds.normalRetryTimesBeforeTriggerWait=3

# Max retry time for IO-Path request
mds.maxRetryMsInIOPath=86400000

# Sleep interval for wait
mds.waitSleepMs=10000

#
#################Metacache Configuration Information################
#

# Obtain the rpc timeout of the leader
metacache.getLeaderTimeOutMS=500

# Obtain the backup request timeout for the leader
metacache.getLeaderBackupRequestMS=100

# The load balancer method used by getleaer backup request
metacache.getLeaderBackupRequestLbName=rr

# Retrieve the number of retries for the leader
metacache.getLeaderRetry=5

# Obtaining the leader interface requires a period of sleep before each retry
metacache.rpcRetryIntervalUS=100000

#
############### Configuration information of the scheduling layer #############
#

# Scheduling layer queue size, with one queue for each file
# The depth of the scheduling queue can affect the overall throughput of the client, as it stores asynchronous IO tasks..
schedule.queueCapacity=1000000

# Number of execution threads in the queue
# What the executing thread needs to do is to retrieve the IO, then send it to the network and return to retrieve the next network task. A task starts from
# The RPC request is approximately (20us-100us) from the time the queue is retrieved to the time it is sent, and 20us is the normal time when it is not necessary to obtain a leader
# If a leader needs to be obtained during sending, the time will be around 100us, and the throughput of one thread will be between 10w-50w
# The performance has met the requirements
schedule.threadpoolSize=1

# To isolate the task queue introduced by the QEMU side thread, as there is only one IO thread on the QEMU side
# When the QEMU side calls the AIO interface, it directly pushes the call to the task queue and returns,
# This way, libcurve does not occupy QEMU's threads and does not block its asynchronous calls
isolation.taskQueueCapacity=1000000

# The size of the task queue thread pool for isolating QEMU threads, with a default value of 1 thread
isolation.taskThreadPoolSize=1


#
################ Configuration related to communication with chunkserver #############
#
# Retrying sleep between OPs with failed read/write interfaces
chunkserver.opRetryIntervalUS=100000

# Number of failed OP retries
chunkserver.opMaxRetry=50

# RPC timeout for communication with chunkserver
chunkserver.rpcTimeoutMS=1000

# Maximum sleep time between retry requests
# Because when the network is congested or the chunkserver is overloaded, it is necessary to increase sleep time
# The maximum time for this is maxRetrySleepIntervalUs
chunkserver.maxRetrySleepIntervalUS=8000000

# The maximum timeout rpc time for retry requests, which follows an exponential backoff strategy
# Because timeout occurs when the network is congested, it is necessary to increase the RPC timeout time
# The maximum time for this is maxTimeoutMS
chunkserver.maxRPCTimeoutMS=16000

# Maximum number of consecutive timeouts for the same chunkserver
# If this value is exceeded, a health check will be conducted, and if the health check fails, it will be marked as unstable
chunkserver.maxStableTimeoutTimes=64
# The timeout of health check requests after consecutive RPC timeouts on chunkserver
chunkserver.checkHealthTimeoutMs=100
#After the number of unstable chunkservers on the same server exceeds this value
#All chunkservers will be marked as unstable
chunkserver.serverStableThreshold=3

# When the underlying chunkserver is under high pressure, unstable may also be triggered
# Due to copyset leader may change, the request timeout time will be set to the default value, resulting in IO hang
# In the case of real downtime, the request will be processed after a certain number of retries
# If you keep trying again, it's not a downtime situation, and at this point, the timeout still needs to enter the exponential backoff logic
# When the number of retries for a request exceeds this value, its timeout must enter exponential backoff
chunkserver.minRetryTimesForceTimeoutBackoff=5

# When an RPC retry exceeds maxRetryTimesBeforeConsiderSuspend
#Record as suspended IO, metric will alarm
chunkserver.maxRetryTimesBeforeConsiderSuspend=20

#
################# File level configuration items #############
#
# libcurve allows for the maximum number of unreturned rpcs in the underlying rpc scheduling, with each file's inflight RPC being independent
global.fileMaxInFlightRPCNum=64

# The maximum sharding KB for file IO distribution to the underlying chunkserver
global.fileIOSplitMaxSizeKB=64

#
################# Log related configuration ###############
#
# Log level INFO=0/WARNING=1/ERROR=2/FATAL=3
global.logLevel=0
# Set the path of the log
global.logPath=/data/log/curve/  # __CURVEADM_TEMPLATE__ ${prefix}/logs __CURVEADM_TEMPLATE__
# In the case of unit testing
# logpath=./runlog/

#
############### metric configuration information #############
#
global.metricDummyServerStartPort=9000

#
# session map file, storing the mapping from filename to path of the opened file
#
global.sessionMapPath=./session_map.json

##### discard configurations #####
# enable/disable discard
discard.enable=false
# discard granularity
discard.granularity=4096
# discard cleanup task delay times in millisecond
discard.taskDelayMs=60000
