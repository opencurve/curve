#
# Global settings
#
# Log level INFO=0/WARNING=1/ERROR=2/FATAL=3
global.ip=127.0.0.1  # __CURVEADM_TEMPLATE__ ${service_addr} __CURVEADM_TEMPLATE__
global.port=8200  # __CURVEADM_TEMPLATE__ ${service_port} __CURVEADM_TEMPLATE__
global.subnet=127.0.0.0/24
global.enable_external_server=true
global.external_ip=127.0.0.1  # __CURVEADM_TEMPLATE__ ${service_external_addr} __CURVEADM_TEMPLATE__
global.external_subnet=127.0.0.0/24
# Chunk size, usually 16MB
# it will be overwritten from chunkfilepool.meta if `chunkfilepool.enable_get_chunk_from_pool` is true
global.chunk_size=16777216
# Chunk metadata page size, usually 4KB
# it will be overwritten from chunkfilepool.meta if `chunkfilepool.enable_get_chunk_from_pool` is true
global.meta_page_size=4096
# chunk's block size, IO requests must align with it, supported value is |512| and |4096|
# it should consist with `block_size` in chunkfilepool.meta_path and `mds.volume.blockSize` in MDS's configurations
# for clone chunk and snapshot chunk, it's also the minimum granularity that each bit represents
# if set to |512|, we need 4096 bytes bitmap for each chunk, so meta_page_size should be 8192 or larger.
# it will be overwritten from chunkfilepool.meta if `chunkfilepool.enable_get_chunk_from_pool` is true
global.block_size=4096

# The maximum allowed location length for clone chunks
global.location_limit=3000

#
# MDS settings
#
# Supports MDS multiple addresses, separated by commas 127.0.0.1:6666, 127.0.0.1:7777
mds.listen.addr=127.0.0.1:6666  # __CURVEADM_TEMPLATE__ ${cluster_mds_addr} __CURVEADM_TEMPLATE__
# Maximum number of retries registered with mds
mds.register_retries=100
# RPC timeout for registering with mds, typically 1000ms
mds.register_timeout=1000
# The interval between sending heartbeat to MDS, usually 10 seconds
mds.heartbeat_interval=10
#Send rpc timeout of heartbeat to mds, usually 1000ms
mds.heartbeat_timeout=5000

#
# Chunkserver settings
#
# Chunkserver home directory
chunkserver.stor_uri=local://./0/  # __CURVEADM_TEMPLATE__ local://${prefix}/data __CURVEADM_TEMPLATE__
# Chunkserver metadata file
chunkserver.meta_uri=local://./0/chunkserver.dat  # __CURVEADM_TEMPLATE__ local://${prefix}/data/chunkserver.dat __CURVEADM_TEMPLATE__
# Disk type
chunkserver.disk_type=nvme
# Raft internal install snapshot bandwidth limit, usually 20MB
chunkserver.snapshot_throttle_throughput_bytes=20971520
# Check cycles are used for more precise bandwidth control, with snapshots ThroughputBytes=100MB,
# Taking check cycles=10 as an example, it can ensure that the bandwidth is 10MB every 1/10 second and does not accumulate, such as the first one
# The bandwidth of 1/10 second is 10MB, but it expires. In the second 1/10 second, only 10MB of bandwidth can be used, and
# Not a bandwidth of 20MB
chunkserver.snapshot_throttle_check_cycles=4
# Limit the number of inflight io, usually 5000
chunkserver.max_inflight_requests=5000

#
# Testing purpose settings
#
test.create_testcopyset=false
test.testcopyset_poolid=666
test.testcopyset_copysetid=888888
test.testcopyset_conf=127.0.0.1:8200:0,127.0.0.1:8201:0,127.0.0.1:8202:0

#
# Copyset settings
#
# lease read switch, default is true(open lease read)
# if false, all requests will propose to raft(log read)
# Enable lease read, usually enabled, otherwise it will revert to log read form
copyset.enable_lease_read=true
# Whether to check the term of office, general inspection
copyset.check_term=true
# Do you want to close the service for raft configuration changes? Generally, it is not closed
copyset.disable_cli=false
copyset.log_applied_task=false
# Raft election timeout, usually 5000ms
copyset.election_timeout_ms=1000
# The snapshot interval for the raft is usually 1800s, which is 30 minutes
copyset.snapshot_interval_s=1800
# Add a node, and the added node first acts like a learner to copy data.
# When the gap with the leader is equal to catchup_margin entries, the leader
# will attempt to commit the configuration change entry (generally, the committed entry
# will definitely be committed and applied). A small catchup_margin can
# ensure that the learner can join the replication group quickly.
copyset.catchup_margin=1000
# Copyset chunk data directory
copyset.chunk_data_uri=local://./0/copysets  # __CURVEADM_TEMPLATE__ local://${prefix}/data/copysets __CURVEADM_TEMPLATE__
# Raft wal log directory
copyset.raft_log_uri=curve://./0/copysets  # __CURVEADM_TEMPLATE__ curve://${prefix}/data/copysets __CURVEADM_TEMPLATE__
# Raft metadata directory
copyset.raft_meta_uri=local://./0/copysets  # __CURVEADM_TEMPLATE__ local://${prefix}/data/copysets __CURVEADM_TEMPLATE__
# Raft snapshot directory
copyset.raft_snapshot_uri=curve://./0/copysets  # __CURVEADM_TEMPLATE__ curve://${prefix}/data/copysets __CURVEADM_TEMPLATE__
# Copyset Recycle Directory
copyset.recycler_uri=local://./0/recycler  # __CURVEADM_TEMPLATE__ local://${prefix}/data/recycler __CURVEADM_TEMPLATE__
# When chunkserver is started, the threshold for copyset concurrent loading is set to 0, indicating no restrictions are imposed
copyset.load_concurrency=10
# chunkserver use how many threads to use copyset complete sync. 
copyset.sync_concurrency=20
# Check if the copyset has completed loading and the maximum number of retries when an exception occurs
copyset.check_retrytimes=3
# The current peer's applied_index and leaderâ€˜s committed_index difference is less than this value
# Then it is determined that the copyset has been loaded successfully
copyset.finishload_margin=2000
# Internal sleep time for loop determination of whether copyset has been loaded and completed
copyset.check_loadmargin_interval_ms=1000
# scan copyset interval
copyset.scan_interval_sec=5
# the size each scan 4MB
copyset.scan_size_byte=4194304
# the follower send scanmap to leader rpc timeout
copyset.scan_rpc_timeout_ms=1000
# the follower send scanmap to leader rpc retry times
copyset.scan_rpc_retry_times=3
# the follower send scanmap to leader rpc retry interval
copyset.scan_rpc_retry_interval_us=100000
# enable O_DSYNC when open chunkfile
copyset.enable_odsync_when_open_chunkfile=true
# sync trigger seconds
copyset.sync_trigger_seconds=25
# sync chunk limit default = 2MB
copyset.sync_chunk_limits=2097152
# 30s if the sum of write > sync_threshold, let the sync_chunk_limits doubled.
copyset.sync_threshold=65536
# check syncing interval
copyset.check_syncing_interval_ms=500

#
# Clone settings
#
# Prohibit the use of curveclient
clone.disable_curve_client=false
# Prohibit the use of s3adapter
clone.disable_s3_adapter=false
# The shard size of the clone, usually 1MB
clone.slice_size=1048576
# Do I need to paste to the local location when reading the clone chunk
# This configuration is not valid for the recover chunk request type
clone.enable_paste=false
# Number of cloned threads
clone.thread_num=10
# Queue depth for cloning
clone.queue_depth=6000
# Curve username
curve.root_username=root
# Curve password
curve.root_password=root_password
# Client configuration file
curve.config_path=conf/cs_client.conf  # __CURVEADM_TEMPLATE__ ${prefix}/conf/cs_client.conf __CURVEADM_TEMPLATE__
#S3 configuration file
s3.config_path=conf/s3.conf  # __CURVEADM_TEMPLATE__ ${prefix}/conf/s3.conf __CURVEADM_TEMPLATE__
# Curve File time to live
curve.curve_file_timeout_s=30

#
# Local FileSystem settings
#
# Whether to enable the use of renameat2, ext4 kernel support starting from 3.15 onwards
fs.enable_renameat2=true

#
# metrics settings
# true means on, false means off
#
metric.onoff=true

#
# Storage engine settings
#
storeng.sync_write=false

#
# QoS settings
#

#
# Concurrent apply module
# The concurrency of concurrent module writing threads is generally 10
wconcurrentapply.size=10
# Queue depth of concurrent module write threads
wconcurrentapply.queuedepth=1
# The concurrency of concurrent module read threads is generally 5
rconcurrentapply.size=5
# Queue depth of concurrent module read threads
rconcurrentapply.queuedepth=1

#
# Chunkfile pool
#
# Whether to enable obtaining chunks from chunkfilepool, usually true
chunkfilepool.enable_get_chunk_from_pool=true
# chunkfilepool directory
chunkfilepool.chunk_file_pool_dir=./0/  # __CURVEADM_TEMPLATE__ ${prefix}/data __CURVEADM_TEMPLATE__
# chunkfilepool meta file path
chunkfilepool.meta_path=./chunkfilepool.meta  # __CURVEADM_TEMPLATE__ ${prefix}/data/chunkfilepool.meta __CURVEADM_TEMPLATE__
# chunkfilepool meta file size
chunkfilepool.cpmeta_file_size=4096
# chunkfilepool get chunk maximum retry count
chunkfilepool.retry_times=5
# Enable clean chunk
chunkfilepool.clean.enable=true
# The bytes per write for cleaning chunk (max: 1MB)
chunkfilepool.clean.bytes_per_write=4096
# The throttle iops for cleaning chunk (4KB/IO)
chunkfilepool.clean.throttle_iops=500

#
# WAL file pool
#
# Does walpool share chunkfilepool? If true, the configuration is invalid starting from the third entry
walfilepool.use_chunk_file_pool=true
# Enable when WALpool and ChunkFilePool are shared, and reserve space for WALpool during capacity allocation
walfilepool.use_chunk_file_pool_reserve=15
# Whether to enable obtaining chunks from walfilepool, usually true
walfilepool.enable_get_segment_from_pool=true
# Walpool directory
walfilepool.file_pool_dir=./0/  # __CURVEADM_TEMPLATE__ ${prefix}/data/walfilepool.meta __CURVEADM_TEMPLATE__
# Walpool Meta File Path
walfilepool.meta_path=./walfilepool.meta  # __CURVEADM_TEMPLATE__ ${prefix}/data/walfilepool.meta __CURVEADM_TEMPLATE__
# Walpool Meta File Size
walfilepool.segment_size=8388608
# WAL metapage size
walfilepool.metapage_size=4096
# WAL filepool metadata file size
walfilepool.meta_file_size=4096
# WAL filepool get chunk maximum retry count
walfilepool.retry_times=5

#
# trash settings
#
# The expiration time for chunkserver to completely delete data for recycling
trash.expire_afterSec=300
# Chunkserver checks the cycle of recycling data expiration time
trash.scan_periodSec=120

# common option
#
# Chunkserver log storage folder
chunkserver.common.logDir=./  # __CURVEADM_TEMPLATE__ ${prefix}/logs __CURVEADM_TEMPLATE__
# In the case of unit testing
# chunkserver.common.logDir=./runlog/
