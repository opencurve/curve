#
################### MDS side configuration information##################
#

# Address information for mds, separated by commas for mds clusters
mds.listen.addr=127.0.0.1:6666

# Register switch with mds during initialization phase, default to on
mds.registerToMDS=true

# RPC timeout for communication with mds
mds.rpcTimeoutMS=500

# The maximum timeout for rpc communication with mds, and the timeout for exponential backoff cannot exceed this value
mds.maxRPCTimeoutMS=2000

# Total retry time for communication with mds
mds.maxRetryMS=8000

# Switch if the number of consecutive retries on the current mds exceeds this limit, which includes the number of timeout retries
mds.maxFailedTimesBeforeChangeMDS=2

# How many renewals are there within a lease period with MDS
mds.refreshTimesPerLease=4

# The mds RPC interface requires a period of sleep before each retry
mds.rpcRetryIntervalUS=100000

# The normal retry times for trigger wait strategy
mds.normalRetryTimesBeforeTriggerWait=3

# Max retry time for IO-Path request
mds.maxRetryMsInIOPath=86400000

# Sleep interval for wait
mds.waitSleepMs=10000

#
################# Metacache Configuration Information################
#

# Obtain the rpc timeout of the leader
metacache.getLeaderTimeOutMS=500

# Retrieve the number of retries for the leader
metacache.getLeaderRetry=5

# Obtaining the leader interface requires a period of sleep before each retry
metacache.rpcRetryIntervalUS=100000

#
###############Configuration information of the scheduling layer#############
#

# Scheduling layer queue size, with one queue for each file
# The depth of the scheduling queue can affect the overall throughput of the client, as it stores asynchronous IO tasks..
schedule.queueCapacity=1000000

# Number of execution threads in the queue
# What the executing thread needs to do is to retrieve the IO, then send it to the network and return to retrieve the next network task. A task starts from
# The RPC request is approximately (20us-100us) from the time the queue is retrieved to the time it is sent, and 20us is the normal time when it is not necessary to obtain a leader
# If a leader needs to be obtained during sending, the time will be around 100us, and the throughput of one thread will be between 10w-50w
# The performance has met the requirements
schedule.threadpoolSize=2

# To isolate the task queue introduced by the QEMU side thread, as there is only one IO thread on the QEMU side
# When the QEMU side calls the AIO interface, it directly pushes the call to the task queue and returns,
# This way, libcurve does not occupy QEMU's threads and does not block its asynchronous calls
isolation.taskQueueCapacity=1000000

# The size of the task queue thread pool for isolating QEMU threads, with a default value of 1 thread
isolation.taskThreadPoolSize=1


#
################Configuration related to communication with chunkserver#############
#
# Retrying sleep between OPs with failed read/write interfaces
chunkserver.opRetryIntervalUS=100000

# Number of failed OP retries
chunkserver.opMaxRetry=2500000

# RPC timeout for communication with chunkserver
chunkserver.rpcTimeoutMS=1000

# Maximum sleep time between retry requests
# Because when the network is congested or the chunkserver is overloaded, it is necessary to increase sleep time
# The maximum time for this is maxRetrySleepIntervalUs
chunkserver.maxRetrySleepIntervalUS=8000000

# The maximum timeout rpc time for retry requests, which follows an exponential backoff strategy
# Because timeout occurs when the network is congested, it is necessary to increase the RPC timeout time
# The maximum time for this is maxTimeoutMS
chunkserver.maxRPCTimeoutMS=8000

# Maximum number of consecutive timeouts for the same chunkserver
# If this value is exceeded, a health check will be conducted, and if the health check fails, it will be marked as unstable
chunkserver.maxStableTimeoutTimes=10
# The timeout of health check requests after consecutive RPC timeouts on chunkserver
chunkserver.checkHealthTimeoutMs=100
# After the number of unstable chunkservers on the same server exceeds this value
# All chunkservers will be marked as unstable
chunkserver.serverStableThreshold=3

# When the underlying chunkserver is under high pressure, unstable may also be triggered
# Due to copyset leader may change, the request timeout time will be set to the default value, resulting in IO hang
# In the case of real downtime, the request will be processed after a certain number of retries
# If you keep trying again, it's not a downtime situation, and at this point, the timeout still needs to enter the exponential backoff logic
# When the number of retries for a request exceeds this value, its timeout must enter exponential backoff
chunkserver.minRetryTimesForceTimeoutBackoff=5

# When an RPC retry exceeds maxRetryTimesBeforeConsiderSuspend
# Record as suspended IO, metric will alarm
chunkserver.maxRetryTimesBeforeConsiderSuspend=20

#
#################File level configuration items#############
#
# Libcurve allows for the maximum number of unreturned rpcs in the underlying rpc scheduling, with each file's inflight RPC being independent
global.fileMaxInFlightRPCNum=128

# The maximum sharding KB for file IO distribution to the underlying chunkserver
global.fileIOSplitMaxSizeKB=64

#
################# Log related configuration###############
#
# enable logging or not
global.logging.enable=True
#
# Log level INFO=0/WARNING=1/ERROR=2/FATAL=3
global.logLevel=0
# Set the path of the log
global.logPath=/data/log/curve/  # __CURVEADM_TEMPLATE__ /curvebs/client/logs __CURVEADM_TEMPLATE__
# In the case of unit testing
# logpath=./runlog/

#
################# Read source volume related configurations###############
#
# Opening fd timeout when reading source volume, closing time 300s
closefd.timeout=300
# When reading the source volume, open the fd backend thread to scan the fdMap every 600 seconds, and close the timeout fd
closefd.timeInterval=600

#
############### Metric configuration information#############
#
global.metricDummyServerStartPort=9000

# Whether to turn off health check: true/turn off, false/do not turn off
global.turnOffHealthCheck=true

#
### throttle config
#
throttle.enable=false

##### discard configurations #####
# enable/disable discard
discard.enable=true
# discard granularity
discard.granularity=4096
# discard cleanup task delay times in millisecond
discard.taskDelayMs=60000

##### chunkserver client option #####
# chunkserver client rpc timeout time
csClientOpt.rpcTimeoutMs=500
# chunkserver client rpc max try
csClientOpt.rpcMaxTry=86400000
# chunkserver client rpc retry interval
csClientOpt.rpcIntervalUs=100000
# chunkserver client rpc max timeout time
csClientOpt.rpcMaxTimeoutMs=8000

##### chunkserver broadcaster option #####
# broad cast max machine num
csBroadCasterOpt.broadCastMaxNum=200
