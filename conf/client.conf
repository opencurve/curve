#
# mds一侧配置信息
#
# mds的地址信息
metaserver_addr=127.0.0.1:6666
# 与mds通信的rpc超时次数
rpcRetryTimes=1000000
# 与MDS一侧保持一个lease时间内多少次续约
refreshTimesPerLease=4
# 同步调用接口rpc超时时间，Open、GetFileInfo等接口
synchronizeRPCTimeoutMS=1000
# 同步调用接口rpc超时次数, 重试加睡眠时间在10s中左右
# mds一侧的lease time可以应该在10s钟以内，如果要改大mds leasetime
# 也要把这个重试加睡眠时间调大，以能够保证重试时可以覆盖一个lease时间
synchronizeRPCRetryTime=100

# 与mds和chunkserver通信的rpc超时时间
rpcTimeoutMs=500

#
# metacache配置信息
#
# 获取leader的rpc超时时间
getLeaderTimeOutMs=500
# 获取leader的重试次数
getLeaderRetry=5
# mds RPC接口和getleader接口每次重试之前需要先睡眠一段时间
retryIntervalUs=100000

#
# request调度层的配置信息
#
#
# 调度层队列大小，每个文件对应一个队列
# 调度队列的深度会影响client端整体吞吐，这个队列存放的是异步IO任务。。
queueCapacity=1000000
# 队列的执行线程数量
# 执行线程所要做的事情就是将IO取出，然后发到网络就返回取下一个网络任务。一个任务从
# 队列取出到发送完rpc请求大概在(20us-100us)，20us是正常情况下不需要获取leader的时候
# 如果在发送的时候需要获取leader，时间会在100us左右，一个线程的吞吐在10w-50w
# 性能已经满足需求
threadpoolSize=1

#
# 为隔离qemu侧线程引入的任务队列，因为qemu一侧只有一个IO线程
# 当qemu一侧调用aio接口的时候直接将调用push到任务队列就返回，
# 这样libcurve不占用qemu的线程，不阻塞其异步调用
taskQueueCapacity=1000000
#
# 任务队列线程池大小, 默认值为1个线程
#
taskThreadPoolSize=1


#
# io发送相关配置
#
# 读写接口失败的OP之间重试睡眠
opRetryIntervalUs=100000
# 失败的OP重试次数
opMaxRetry=4294967290
# 开启基于appliedindex的读，用于性能优化
enableAppliedIndexRead=1
# 下发IO最大的分片KB
ioSplitMaxSizeKB=64
# libcurve底层rpc调度允许最大的未返回rpc数量，每个文件的inflight RPC独立
maxInFlightRPCNum=64
# 重试请求之间睡眠最长时间
# 因为当网络拥塞的时候或者chunkserver出现过载的时候，需要增加睡眠时间
# 这个时间最大为maxRetrySleepIntervalUs
maxRetrySleepIntervalUs=8000000
# 重试请求的超时rpc时间最大值，超时时间会遵循指数退避策略
# 因为当网络拥塞的时候出现超时，需要增加RPC超时时间
# 这个时间最大为maxTimeoutMS
maxTimeoutMS=8000
# 同一个chunkserver连续超时上限次数，如果超过这个值，就会设置为unstable状态
maxStableChunkServerTimeoutTimes=10

# 当一个rpc重试超过次数maxRetryTimesBeforeConsiderSuspend的时候
# 记为悬挂IO，metric会报警
maxRetryTimesBeforeConsiderSuspend=20

#
# log相关配置
#
# log等级 INFO=0/WARNING=1/ERROR=2/FATAL=3
loglevel=0
# 设置log的路径
logpath=/var/log/
# 单元测试情况下
# logpath=./runlog/

#
# metric 配置信息
#
dummyServerStartPort=9000

#
# 初始化阶段向mds注册开关，默认为开
#
registerToMDS=true

#
# session map文件，存储打开文件的filename到path的映射
#
sessionMapPath=./session_map.json
