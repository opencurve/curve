#
# Mds service port
#
mds.listen.addr=127.0.0.1:6666  #__CURVEADM_TEMPLATE__ ${service_addr}:${service_port} __CURVEADM_TEMPLATE__
mds.dummy.listen.port=6667  # __CURVEADM_TEMPLATE__ ${service_dummy_port} __CURVEADM_TEMPLATE__
global.subnet=127.0.0.0/24
global.port=6666  # __CURVEADM_TEMPLATE__ ${service_port} __CURVEADM_TEMPLATE__

#
# ETCD related configurations
#
# ETCD address
mds.etcd.endpoint=127.0.0.1:2379  # __CURVEADM_TEMPLATE__ ${cluster_etcd_addr} __CURVEADM_TEMPLATE__
# The timeout period for establishing a connection with a client
mds.etcd.dailtimeoutMs=5000
# The timeout period for client to perform put/get/txn and other operations
mds.etcd.operation.timeoutMs=5000
# The number of times a client operation failed and can be retried
mds.etcd.retry.times=3
# wait dlock timeout
mds.etcd.dlock.timeoutMs=10000
# dlock lease timeout
mds.etcd.dlock.ttlSec=10
# etcd auth options
etcd.auth.enable=false
etcd.auth.username=
etcd.auth.password=

#
# Configuration related to segment allocation statistics
#
# The interval between persisting data in memory to ETCD, in milliseconds
mds.segment.alloc.periodic.persistInterMs=10000
# The retry interval in ms in case of an error
mds.segment.alloc.retryInterMs=1000

mds.segment.discard.scanIntevalMs=5000


# During the leader election, a session is created in seconds (the unit of the value for the interface of the go code is s)
# This value is related to the ETCD cluster selection timeout
# The server side of ETCD limits this value to a minimum of 1.5 * election timeout
# Suggest setting the ETCD cluster selection timeout to 1 second
mds.leader.sessionInterSec=5
# The timeout period for the leader election. If it is 0 and the election is unsuccessful, it will continue to block. If it is greater than 0, it will be in the selectionTimeoutMs time
# If a leader is not selected, an error will be returned
mds.leader.electionTimeoutMs=0

#
# schedule related configurations
#
# copysetScheduler switch
mds.enable.copyset.scheduler=true
# leaderScheduler switch
mds.enable.leader.scheduler=true
# recoverScheduler switch
mds.enable.recover.scheduler=true
# replicaScheduler switch
mds.enable.replica.scheduler=true
# Scan scheduler switch
mds.enable.scan.scheduler=true
# copysetScheduler round interval, measured in seconds
mds.copyset.scheduler.intervalSec=5
# replicaScheduler round interval, measured in seconds
mds.replica.scheduler.intervalSec=5
# leaderScheduler round interval, measured in seconds
mds.leader.scheduler.intervalSec=30
# recoverScheduler round interval, measured in seconds
mds.recover.scheduler.intervalSec=5
# Scan scheduler run interval (seconds)
mds.scan.scheduler.intervalSec=60
# The concurrency of operators on each disk
mds.schduler.operator.concurrent=1
# The leader changes the timeout time, and after the timeout, the mds removes the operator from memory
mds.schduler.transfer.limitSec=60
# Reduce the replica timeout by one, and after the timeout, the mds removes the operator from memory
mds.scheduler.remove.limitSec=300
# Add a replica timeout, after which the mds removes the operator from memory
mds.scheduler.add.limitSec=1800
# change a replica timeout, after which the mds removes the operator from memory
mds.scheduler.change.limitSec=1800
# Scan operator timeout (seconds)
mds.scheduler.scan.limitSec=180
# The range of copyset quantity cannot exceed the percentage of the mean
mds.scheduler.copysetNumRangePercent=0.05
# The scatter width of the copyset on chunkserver cannot exceed the percentage of the minimum value
mds.schduler.scatterWidthRangePerent=0.2
# There are more than a certain number of chunkservers offline on a server, and no recovery will be performed
mds.chunkserver.failure.tolerance=3
# chunkserver starts coolingTimeSec_ Only then can it be used as a target leader, with the unit of s
# TODO(lixiaocui): Continuation is to some extent related to the time interval of the snapshot
mds.scheduler.chunkserver.cooling.timeSec=1800
# ScanScheduler: scan start hour in one day ([0-23])
mds.scheduler.scan.startHour=0
# ScanScheduler: scan end hour in one day ([0-23])
mds.scheduler.scan.endHour=6
# ScanScheduler: scan interval for the same copyset (seconds)
mds.scheduler.scan.intervalSec=86400
# ScanScheduler: maximum number of scan copysets at the same time for every logical pool
mds.scheduler.scan.concurrent.per.pool=10
# ScanScheduler: maximum number of scan copysets at the same time for every chunkserver
mds.scheduler.scan.concurrent.per.chunkserver=1

#
# Heartbeat related configuration, in ms
#
# Heartbeat interval between chunkserver and mds
mds.heartbeat.intervalMs=10000
# The time of heartbeat miss between chunkserver and mds
mds.heartbeat.misstimeoutMs=30000
# Mds marked offlinetimeout as offline after heartbeat miss
mds.heartbeat.offlinetimeoutMs=1800000
# After starting the mds, delay for a certain period of time to guide chunkserver in deleting physical data
# The reason for delayed deletion is noted in the code
mds.heartbeat.clean_follower_afterMs=1200000

#
# Namespace cache related
#
# The cache size of namestorage, where 0 indicates no caching
# Based on a minimum space budget of 10GB per file. Including oversold (2x)
# Number of files=5PB/10GB ~= 524288 files
# sizeof(namespace object) * 524288 ~= 89Byte * 524288 ~= 44MB space
# 16MB chunk size, 1 segment 1GB
# sizeof(segment object) * 2621440 ~= (32+(1024/16)*12) * 2621440~=1.95 GB
# Data volume: about 3GB
# Record quantity: 524288+2621440 ~= about 300w
mds.cache.count=100000

#
# mds file record settings
#
# Mds file records expiration time, in units of us
mds.file.expiredTimeUs=5000000
# MDS backend scanning thread scanning file record interval time, unit: us
mds.file.scanIntevalTimeUs=500000

#
# auth settings
#
# Root User Password
mds.auth.rootUserName=root
mds.auth.rootPassword=root_password

#
# file lock setting
#
# File lock bucket size for mds
mds.filelock.bucketNum=8

#
# topology config
#
# The time interval for Toplogy to periodically refresh into the database
mds.topology.TopologyUpdateToRepoSec=60
# Request timeout for creating all copysets on chunkserver
mds.topology.CreateCopysetRpcTimeoutMs=10000
# Request to create copyset on chunkserver retry count
mds.topology.CreateCopysetRpcRetryTimes=20
# Request to create copyset on chunkserver retry interval
mds.topology.CreateCopysetRpcRetrySleepTimeMs=1000
# Topology module refresh metric interval
mds.topology.UpdateMetricIntervalSec=10
# It is related to the settings of mds.chunkserver.failure.tolerance. A standard configuration for a zone is 20 nodes, and if 3 nodes are allowed to fail over,
# So the remaining 17 machines need to carry the space of the original 20 machines, 17/20=0.85. Even if the usage exceeds this value, they will no longer be allocated to this pool,
# There are two specific situations: when chunkfilepool is not used, the physical pool limits the percentage of usage, and when chunkfilepool is used for chunkfilepool allocation, it is necessary to reserve failover space,
mds.topology.PoolUsagePercentLimit=85
# Multi pool selection pool strategy 0:Random, 1:Weight
mds.topology.choosePoolPolicy=0
# enable LogicalPool ALLOW/DENY status
mds.topology.enableLogicalPoolStatus=false

#
# copyset config
# Default value, not enabled when 0
#
#Generate copyset retry count
mds.copyset.copysetRetryTimes=10
# The maximum variance that the scatterWidth of all chunkservers must meet
mds.copyset.scatterWidthVariance=0
# The maximum standard deviation that the scatterWidth of all chunkservers must meet
mds.copyset.scatterWidthStandardDevation=0
# The maximum range that the scatterWidth of all chunkservers needs to meet
mds.copyset.scatterWidthRange=0
# Percentage of deviation from the mean scatterWidth of all chunk servers.
# Setting a large percentage deviation for scatterWidth can result in some machines having scatterWidth values that are too small, affecting machine recovery time and reducing cluster reliability.
# Additionally, it can lead to some machines having excessively large scatterWidth values, causing certain chunk server's copysets to be scattered across various machines. 
# Once data is written to these servers, the ones with larger scatterWidth become hotspots. Setting the percentage deviation for scatterWidth too small requires a higher level of scatterWidth
# uniformity and copyset algorithm precision, potentially resulting in suboptimal algorithm results. 
# It is recommended to set the value to 20.
mds.copyset.scatterWidthFloatingPercentage=20

#
# curvefs config
#
# The default chunk size for curvefs is 16MB = 16*1024*1024 = 16777216
mds.curvefs.defaultChunkSize=16777216
# The default segment size for curves is 1GB = 1*1024*1024*1024 = 1073741824
mds.curvefs.defaultSegmentSize=1073741824
# The default minimum file size for curvefs is 10GB = 10*1024*1024*1024 = 10737418240
mds.curvefs.minFileLength=10737418240
# The default maximum file size for curvefs is 20TB = 20*1024*1024*1024*1024 = 21990232555520
mds.curvefs.maxFileLength=21990232555520
# smallest read/write unit for volume, support |512| and |4096|
mds.curvefs.blockSize=4096

#
# chunkseverclient config
#
# RPC timeout
mds.chunkserverclient.rpcTimeoutMs=500
# RPC retry count
mds.chunkserverclient.rpcRetryTimes=5
# RPC retry interval
mds.chunkserverclient.rpcRetryIntervalMs=500
# The maximum number of retries from each chunkserver getleader in the copyset
mds.chunkserverclient.updateLeaderRetryTimes=5
# The interval between each round of each chunkserver getleader in the copyset must be greater than the time for selecting the master in the raft
mds.chunkserverclient.updateLeaderRetryIntervalMs=5000

#
# snapshotclone config
#
# snapshot clone server address
mds.snapshotcloneclient.addr=127.0.0.1:5555  # __CURVEADM_TEMPLATE__ ${cluster_snapshotclone_proxy_addr} __CURVEADM_TEMPLATE__

#
# common options
#
# Log storage folder
mds.common.logDir=./  # __CURVEADM_TEMPLATE__ ${prefix}/logs __CURVEADM_TEMPLATE__
# In the case of unit testing
# mds.common.logDir=./runlog/

#
#### throttle options ####
#
# iops
mds.throttle.iopsMin=2000
mds.throttle.iopsMax=26000
mds.throttle.iopsPerGB=30
# bps
mds.throttle.bpsMinInMB=120
mds.throttle.bpsMaxInMB=260
mds.throttle.bpsPerGBInMB=0.3

#
## poolset rules
#
# for backward compatibility, rules are applied for select poolset when creating file
#
# for example
#   mds.poolset.rules=/dir1/:poolset1;/dir2/:poolset2;/dir1/sub/:sub
#
# when creating file reqeust doesn't have poolset, above rules are used to select poolset
# - if filename is /dir1/file, then poolset1 is select
# - if filename is /dir1/sub/file, then sub is select
mds.poolset.rules=
